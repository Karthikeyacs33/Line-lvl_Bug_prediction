{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657d01d7-cb2d-4dfc-ac71-5f5df423ae88",
   "metadata": {},
   "source": [
    "# DeepLineDP Model for python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1228b86-c91c-4c3c-b808-ceabf8bdcdb8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014f4efa-b688-4b82-9939-840564bad5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os, re, time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "import more_itertools\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "\n",
    "from sklearn.utils import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cffd9-d7f4-4cc0-a34e-4544931202a0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60f84cd-9f0d-4aad-a080-6f239c872848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>content</th>\n",
       "      <th>methods</th>\n",
       "      <th>lines</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-11 14:07:40-01:00</td>\n",
       "      <td>01b498ec5109da22bf1b79d86efaecf45426ad51</td>\n",
       "      <td>django-rest-framework</td>\n",
       "      <td>rest_framework\\schemas.py</td>\n",
       "      <td>b'from importlib import import_module\\n\\nfrom ...</td>\n",
       "      <td>[insert_into, add_categories, get_api_endpoint...</td>\n",
       "      <td>[69, 87, 89, 92, 93, 95, 96, 97, 98, 99, 100, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-24 17:47:46-01:00</td>\n",
       "      <td>007bc310840d9cd5b37983a0c6ba82bd9e551c26</td>\n",
       "      <td>poetry</td>\n",
       "      <td>poetry\\inspection\\info.py</td>\n",
       "      <td>b'import glob\\nimport logging\\nimport os\\nimpo...</td>\n",
       "      <td>[_find_dist_info, from_directory, from_sdist, ...</td>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-16 03:51:12-01:00</td>\n",
       "      <td>0786e84a33155ebc8d8d3502e3a7f3060b86a4ec</td>\n",
       "      <td>scrapy</td>\n",
       "      <td>scrapy\\contrib\\pipeline\\files.py</td>\n",
       "      <td>b'\"\"\"\\nFiles Pipeline\\n\"\"\"\\n\\nimport hashlib\\n...</td>\n",
       "      <td>[file_downloaded]</td>\n",
       "      <td>[14, 15, 16, 17, 18, 264]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-15 11:29:05-01:00</td>\n",
       "      <td>00035672a460b6eb5442d2837bc783f8af28c6f3</td>\n",
       "      <td>django</td>\n",
       "      <td>django\\db\\models\\fields\\__init__.py</td>\n",
       "      <td>b'import collections.abc\\nimport copy\\nimport ...</td>\n",
       "      <td>[get_choices]</td>\n",
       "      <td>[828, 829, 830, 832]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-08 09:46:09-01:00</td>\n",
       "      <td>00e7e12a3a412ea386806d5d4eeaed345e912940</td>\n",
       "      <td>black</td>\n",
       "      <td>src\\black\\linegen.py</td>\n",
       "      <td>b'\"\"\"\\nGenerating lines of code.\\n\"\"\"\\nfrom fu...</td>\n",
       "      <td>[visit_STRING]</td>\n",
       "      <td>[229, 230, 231]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                    commit  \\\n",
       "0 2016-08-11 14:07:40-01:00  01b498ec5109da22bf1b79d86efaecf45426ad51   \n",
       "1 2020-07-24 17:47:46-01:00  007bc310840d9cd5b37983a0c6ba82bd9e551c26   \n",
       "2 2014-07-16 03:51:12-01:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
       "3 2019-08-15 11:29:05-01:00  00035672a460b6eb5442d2837bc783f8af28c6f3   \n",
       "4 2021-06-08 09:46:09-01:00  00e7e12a3a412ea386806d5d4eeaed345e912940   \n",
       "\n",
       "                    repo                             filepath  \\\n",
       "0  django-rest-framework            rest_framework\\schemas.py   \n",
       "1                 poetry            poetry\\inspection\\info.py   \n",
       "2                 scrapy     scrapy\\contrib\\pipeline\\files.py   \n",
       "3                 django  django\\db\\models\\fields\\__init__.py   \n",
       "4                  black                 src\\black\\linegen.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  b'from importlib import import_module\\n\\nfrom ...   \n",
       "1  b'import glob\\nimport logging\\nimport os\\nimpo...   \n",
       "2  b'\"\"\"\\nFiles Pipeline\\n\"\"\"\\n\\nimport hashlib\\n...   \n",
       "3  b'import collections.abc\\nimport copy\\nimport ...   \n",
       "4  b'\"\"\"\\nGenerating lines of code.\\n\"\"\"\\nfrom fu...   \n",
       "\n",
       "                                             methods  \\\n",
       "0  [insert_into, add_categories, get_api_endpoint...   \n",
       "1  [_find_dist_info, from_directory, from_sdist, ...   \n",
       "2                                  [file_downloaded]   \n",
       "3                                      [get_choices]   \n",
       "4                                     [visit_STRING]   \n",
       "\n",
       "                                               lines  target  \n",
       "0  [69, 87, 89, 92, 93, 95, 96, 97, 98, 99, 100, ...       1  \n",
       "1  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 2...       1  \n",
       "2                          [14, 15, 16, 17, 18, 264]       1  \n",
       "3                               [828, 829, 830, 832]       1  \n",
       "4                                    [229, 230, 231]       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_line_random = '.'\n",
    "\n",
    "train_df = pd.read_parquet(f'{path_to_line_random}/train.parquet.gzip')\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "test_df = pd.read_parquet(f'{path_to_line_random}/test.parquet.gzip')\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df['target'] = train_df['lines'].apply(lambda line : 0 if len(line) == 0 else 1)\n",
    "test_df['target'] = test_df['lines'].apply(lambda line : 0 if len(line) == 0 else 1)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95340d06-045e-457e-acd6-004b5f2b9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = train_df[train_df['target'] == 1].sample(75, random_state=42)\n",
    "train_df_0 = train_df[train_df['target'] == 0].sample(75, random_state=42)\n",
    "\n",
    "# Combine the DataFrames\n",
    "train_df = pd.concat([train_df_1, train_df_0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3e3d1b-df3d-406b-a360-5f8ae9b6894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:55: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:55: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\mvska\\AppData\\Local\\Temp\\ipykernel_31684\\3109217446.py:55: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  code_line = re.sub('\\b\\d+\\b','',code_line)\n"
     ]
    }
   ],
   "source": [
    "# data_root_dir = '../datasets/original/'\n",
    "# save_dir = \"../datasets/preprocessed_data/\"\n",
    "\n",
    "char_to_remove = ['+','-','*','/','=','++','--','\\\\','<str>','<char>','|','&','!']\n",
    "\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# file_lvl_dir = data_root_dir+'File-level/'\n",
    "# line_lvl_dir = data_root_dir+'Line-level/'\n",
    "\n",
    "\n",
    "def is_comment_line(code_line, comments_list):\n",
    "    '''\n",
    "        input\n",
    "            code_line (string): source code in a line\n",
    "            comments_list (list): a list that contains every comments\n",
    "        output\n",
    "            boolean value\n",
    "    '''\n",
    "\n",
    "    code_line = code_line.strip()\n",
    "\n",
    "    if len(code_line) == 0:\n",
    "        return False\n",
    "    elif code_line.startswith('#'):\n",
    "        return True\n",
    "    elif code_line in comments_list:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def is_empty_line(code_line):\n",
    "    '''\n",
    "        input\n",
    "            code_line (string)\n",
    "        output\n",
    "            boolean value\n",
    "    '''\n",
    "\n",
    "    if len(code_line.strip()) == 0:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def preprocess_code_line(code_line):\n",
    "    '''\n",
    "        input\n",
    "            code_line (string)\n",
    "    '''\n",
    "\n",
    "    code_line = re.sub(\"\\'\\'\", \"\\'\", code_line)\n",
    "    code_line = re.sub(\"\\\".*?\\\"\", \"<str>\", code_line)\n",
    "    code_line = re.sub(\"\\'.*?\\'\", \"<char>\", code_line)\n",
    "    code_line = re.sub('\\b\\d+\\b','',code_line)\n",
    "    code_line = re.sub(\"\\\\[.*?\\\\]\", '', code_line)\n",
    "    code_line = re.sub(\"[\\\\.|,|:|;|{|}|(|)]\", ' ', code_line)\n",
    "\n",
    "    for char in char_to_remove:\n",
    "        code_line = code_line.replace(char,' ')\n",
    "\n",
    "    code_line = code_line.strip()\n",
    "\n",
    "    return code_line\n",
    "\n",
    "def preprocess_code(code_str):\n",
    "    '''\n",
    "        input\n",
    "            code_str (multi line str)\n",
    "    '''\n",
    "    if(code_str is None):\n",
    "        return ''\n",
    "    code_str = code_str.decode(\"latin-1\")\n",
    "    code_lines = code_str.splitlines()\n",
    "\n",
    "    preprocess_code_lines = []\n",
    "    is_comments = []\n",
    "    is_blank_line = []\n",
    "\n",
    "    # multi-line comments\n",
    "    comments = re.findall(r'(\"\"\"(.*?)\"\"\")|(\\'\\'\\'(.*?)\\'\\'\\')', code_str, re.DOTALL)\n",
    "    comments_temp = []\n",
    "    for tup in comments:\n",
    "        temp = ''\n",
    "        for s in tup:\n",
    "            temp += s\n",
    "        comments_temp.append(temp)\n",
    "    comments_str = '\\n'.join(comments_temp)\n",
    "    comments_list = comments_str.split('\\n')\n",
    "\n",
    "    for l in code_lines:\n",
    "        l = l.strip()\n",
    "        is_comment = is_comment_line(l,comments_list)\n",
    "        is_comments.append(is_comment)\n",
    "\n",
    "        if not is_comment:\n",
    "            l = preprocess_code_line(l)\n",
    "\n",
    "        preprocess_code_lines.append(l)\n",
    "\n",
    "    return ' \\n '.join(preprocess_code_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8045a182-c766-4b08-b8d6-953d25a90490",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content'] = train_df['content'].apply(preprocess_code)\n",
    "train_df.to_csv('./preprocessed_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d499e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>content</th>\n",
       "      <th>methods</th>\n",
       "      <th>lines</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-23 23:09:24-01:00</td>\n",
       "      <td>a85808e3257c8b5ae906ecc7e816ffea19ce52b2</td>\n",
       "      <td>core</td>\n",
       "      <td>homeassistant\\components\\derivative\\sensor.py</td>\n",
       "      <td>\\n from decimal import Decimal  DecimalExcept...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14, 15, 16, 17, 49, 50, 51, 52, 53, 54, 67]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-02 02:03:19-01:00</td>\n",
       "      <td>42e2b801fe4cdb9e6fcff3c53b7d732bde59282b</td>\n",
       "      <td>airflow</td>\n",
       "      <td>airflow\\www\\views.py</td>\n",
       "      <td># \\n # Licensed to the Apache Software Foundat...</td>\n",
       "      <td>[_mark_task_instance_state, confirm]</td>\n",
       "      <td>[2224, 2286, 3599]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31 20:48:32-01:00</td>\n",
       "      <td>6cf57da89f199c5749974e141b4dba536bd57ee3</td>\n",
       "      <td>core</td>\n",
       "      <td>homeassistant\\components\\blink\\__init__.py</td>\n",
       "      <td>\\n import asyncio \\n from copy import deepcop...</td>\n",
       "      <td>[_reauth_flow_wrapper]</td>\n",
       "      <td>[19, 53]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-07 02:48:45-01:00</td>\n",
       "      <td>f7e1040236e088f4a0b5c725461cdf0eed80b068</td>\n",
       "      <td>lightning</td>\n",
       "      <td>pytorch_lightning\\trainer\\distrib_parts.py</td>\n",
       "      <td>\"\"\" \\n Lightning makes multi-gpu training and ...</td>\n",
       "      <td>[parse_gpu_ids]</td>\n",
       "      <td>[167, 168, 169, 170, 171, 172, 173, 174, 175, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-07 10:51:16-01:00</td>\n",
       "      <td>c1e51ef486cec17b69727b47452a34a7796d5677</td>\n",
       "      <td>ansible</td>\n",
       "      <td>lib\\ansible\\modules\\source_control\\github_webh...</td>\n",
       "      <td>#!/usr/bin/python \\n # \\n # Copyright: (c) 201...</td>\n",
       "      <td>[main]</td>\n",
       "      <td>[89, 94, 97, 131, 132]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                    commit  \\\n",
       "0 2020-02-23 23:09:24-01:00  a85808e3257c8b5ae906ecc7e816ffea19ce52b2   \n",
       "1 2022-03-02 02:03:19-01:00  42e2b801fe4cdb9e6fcff3c53b7d732bde59282b   \n",
       "2 2021-03-31 20:48:32-01:00  6cf57da89f199c5749974e141b4dba536bd57ee3   \n",
       "3 2019-12-07 02:48:45-01:00  f7e1040236e088f4a0b5c725461cdf0eed80b068   \n",
       "4 2019-02-07 10:51:16-01:00  c1e51ef486cec17b69727b47452a34a7796d5677   \n",
       "\n",
       "        repo                                           filepath  \\\n",
       "0       core      homeassistant\\components\\derivative\\sensor.py   \n",
       "1    airflow                               airflow\\www\\views.py   \n",
       "2       core         homeassistant\\components\\blink\\__init__.py   \n",
       "3  lightning         pytorch_lightning\\trainer\\distrib_parts.py   \n",
       "4    ansible  lib\\ansible\\modules\\source_control\\github_webh...   \n",
       "\n",
       "                                             content  \\\n",
       "0   \\n from decimal import Decimal  DecimalExcept...   \n",
       "1  # \\n # Licensed to the Apache Software Foundat...   \n",
       "2   \\n import asyncio \\n from copy import deepcop...   \n",
       "3  \"\"\" \\n Lightning makes multi-gpu training and ...   \n",
       "4  #!/usr/bin/python \\n # \\n # Copyright: (c) 201...   \n",
       "\n",
       "                                methods  \\\n",
       "0                                    []   \n",
       "1  [_mark_task_instance_state, confirm]   \n",
       "2                [_reauth_flow_wrapper]   \n",
       "3                       [parse_gpu_ids]   \n",
       "4                                [main]   \n",
       "\n",
       "                                               lines  target  \n",
       "0       [14, 15, 16, 17, 49, 50, 51, 52, 53, 54, 67]       1  \n",
       "1                                 [2224, 2286, 3599]       1  \n",
       "2                                           [19, 53]       1  \n",
       "3  [167, 168, 169, 170, 171, 172, 173, 174, 175, ...       1  \n",
       "4                             [89, 94, 97, 131, 132]       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5056a-54a0-4767-8e9c-78f06cd14c60",
   "metadata": {},
   "source": [
    "## Token Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95ca7b35-3b5a-4d88-8416-2f382e7dff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50\n",
    "def prepare_code2d(code_list):\n",
    "    '''\n",
    "        input\n",
    "            code_list (list): list that contains code each line (in str format)\n",
    "        output\n",
    "            code2d (nested list): a list that contains list of tokens with padding by '<pad>'\n",
    "    '''\n",
    "    # content to list(content)\n",
    "    code_list = str(code_list)\n",
    "    code_list = code_list.splitlines()\n",
    "    code2d = []\n",
    "\n",
    "    for c in code_list:\n",
    "        c = re.sub('\\\\s+',' ',c)\n",
    "\n",
    "        c = c.lower()\n",
    "\n",
    "        token_list = c.strip().split()\n",
    "        total_tokens = len(token_list)\n",
    "\n",
    "        token_list = token_list[:max_seq_len]\n",
    "\n",
    "        if total_tokens < max_seq_len:\n",
    "            token_list = token_list + ['<pad>']*(max_seq_len-total_tokens)\n",
    "\n",
    "        code2d.append(token_list)\n",
    "\n",
    "    return code2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03ae0f43-3066-4372-ad99-4d808aade4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content'] = train_df['content'].apply(prepare_code2d)\n",
    "train_df.to_csv('./token2d_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ec76f0-9700-4e85-a494-3fb530a76df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.head(100)\n",
    "test_df['content'] = test_df['content'].apply(preprocess_code)\n",
    "test_df['content'] = test_df['content'].apply(prepare_code2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e73527d3-f3f5-4c17-b381-d6eeb03b5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code3d_and_label(df):\n",
    "    '''\n",
    "        input\n",
    "            df (DataFrame): a dataframe from get_df()\n",
    "        output\n",
    "            code3d (nested list): a list of code2d from prepare_code2d()\n",
    "            all_file_label (list): a list of file-level label\n",
    "    '''\n",
    "    code_3d = prepare_code2d(train_df['content'].to_numpy())\n",
    "    all_file_label = train_df['target'].to_numpy().tolist()\n",
    "    return code_3d, all_file_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d17b965-36f3-4501-a3dd-1f08182be238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(embedding_dim = 50):\n",
    "\n",
    "    w2v_path = './word2vec'\n",
    "\n",
    "    save_path = w2v_path+'/'+'w2v'+str(embedding_dim)+'dim.bin'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print('word2vec model at {} is already exists'.format(save_path))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(w2v_path):\n",
    "        os.makedirs(w2v_path)\n",
    "\n",
    "    # train_df = pd.read_csv('./token2d_train_df.csv') #uncomment to load saved file\n",
    "\n",
    "    train_code_3d, _ = get_code3d_and_label(train_df)\n",
    "\n",
    "    all_texts = list(more_itertools.collapse(train_code_3d[:],levels=2))\n",
    "\n",
    "    word2vec = Word2Vec(all_texts,vector_size=embedding_dim, min_count=1,sorted_vocab=1)\n",
    "\n",
    "    word2vec.save(save_path)\n",
    "    print('save word2vec model at path {} done'.format(save_path))\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e054fd5-a476-47d2-9ea5-ffc46ef0fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model at ./word2vec/w2v50dim.bin is already exists\n"
     ]
    }
   ],
   "source": [
    "word2vec = train_word2vec_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada5dd2-2ae6-436f-b8e3-cf78a1a1a3c2",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cf999b9-0076-4070-a61b-42732896ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_x_vec(code_3d, word2vec):\n",
    "    x_vec = [[[\n",
    "        word2vec.wv.key_to_index[token] if token in word2vec.wv.key_to_index else len(word2vec.wv.key_to_index)\n",
    "        for token in text\n",
    "    ] for text in texts] for texts in code_3d]\n",
    "    return x_vec\n",
    "\n",
    "\n",
    "def pad_code(code_list_3d, max_sent_len, max_seq_len, limit_sent_len=True, mode='train'):\n",
    "  padded = []\n",
    "\n",
    "  for file in code_list_3d:\n",
    "    sent_list = []\n",
    "    for line in file:\n",
    "      new_line = line\n",
    "      # Truncate if line is longer than max_seq_len\n",
    "      if len(line) > max_seq_len:\n",
    "        new_line = line[:max_seq_len]\n",
    "          #edited here ..just trying\n",
    "      elif len(line) <= max_seq_len:\n",
    "          new_line = line + [0] * (max_seq_len - len(new_line))\n",
    "      sent_list.append(new_line)\n",
    "\n",
    "    # Pad the entire file (all sentences) to max_sent_len with zeros\n",
    "    padded_file = sent_list + [[0] * max_seq_len for _ in range(max_sent_len - len(sent_list))]\n",
    "\n",
    "    # If in training mode and `limit_sent_len` is True, keep only the first max_sent_len sentences\n",
    "    if mode == 'train' and limit_sent_len:\n",
    "        padded_file = padded_file[:max_sent_len]\n",
    "      \n",
    "    padded.append(padded_file)\n",
    "    # print(padded_file)\n",
    "  return padded\n",
    "\n",
    "\n",
    "def get_w2v_weight_for_deep_learning_models(word2vec_model, embed_dim):\n",
    "    word2vec_weights = torch.FloatTensor(word2vec_model.wv.vectors)\n",
    "    # add zero vector for unknown tokens\n",
    "    word2vec_weights = torch.cat((word2vec_weights, torch.zeros(1,embed_dim)))\n",
    "    return word2vec_weights\n",
    "    \n",
    "def get_dataloader(code_vec, label_list, batch_size, max_sent_len):\n",
    "  y_tensor = torch.FloatTensor([label for label in label_list])\n",
    "  \n",
    "  # Ensure padding happens to max_sent_len\n",
    "  code_vec_pad = pad_code(code_vec, max_sent_len,max_seq_len)\n",
    "  \n",
    "  # Print shapes for debugging (optional)\n",
    "  print(f\"code_vec shape: {len(code_vec)}\")\n",
    "  print(f\"Y_tensor shape: {len(y_tensor)}\")\n",
    "  print(f\"code_vec_pad shape: {len(code_vec_pad)}\")\n",
    "  \n",
    "  tensor_dataset = TensorDataset(torch.tensor(code_vec_pad), y_tensor)\n",
    "  dl = DataLoader(tensor_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "  return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f2f42-1548-40b7-8d55-70a5020cff7d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447f848b-c7c8-4d7b-bd9f-ee2fdedfac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model structure\n",
    "class HierarchicalAttentionNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim, word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
    "        \"\"\"\n",
    "        vocab_size: number of words in the vocabulary of the model\n",
    "        embed_dim: dimension of word embeddings\n",
    "        word_gru_hidden_dim: dimension of word-level GRU; biGRU output is double this size\n",
    "        sent_gru_hidden_dim: dimension of sentence-level GRU; biGRU output is double this size\n",
    "        word_gru_num_layers: number of layers in word-level GRU\n",
    "        sent_gru_num_layers: number of layers in sentence-level GRU\n",
    "        word_att_dim: dimension of word-level attention layer\n",
    "        sent_att_dim: dimension of sentence-level attention layer\n",
    "        use_layer_norm: whether to use layer normalization\n",
    "        dropout: dropout rate; 0 to not use dropout\n",
    "        \"\"\"\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.sent_attention = SentenceAttention(\n",
    "            vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
    "            word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2 * sent_gru_hidden_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.use_layer_nome = use_layer_norm\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, code_tensor):\n",
    "        \n",
    "        code_lengths = []\n",
    "        sent_lengths = []\n",
    "\n",
    "        for file in code_tensor:\n",
    "            code_line = []\n",
    "            code_lengths.append(len(file))\n",
    "            for line in file:\n",
    "                code_line.append(len(line))\n",
    "            sent_lengths.append(code_line)\n",
    "        \n",
    "        code_tensor = code_tensor.type(torch.LongTensor)\n",
    "        code_lengths = torch.tensor(code_lengths).type(torch.LongTensor)\n",
    "        sent_lengths = torch.tensor(sent_lengths).type(torch.LongTensor)\n",
    "        \n",
    "        code_embeds, word_att_weights, sent_att_weights, sents = self.sent_attention(code_tensor, code_lengths, sent_lengths)\n",
    "\n",
    "        scores = self.fc(code_embeds)\n",
    "        final_scrs = self.sig(scores)\n",
    "\n",
    "        return final_scrs, word_att_weights, sent_att_weights, sents\n",
    "\n",
    "class SentenceAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentence-level attention module. Contains a word-level attention module.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
    "                word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
    "        super(SentenceAttention, self).__init__()\n",
    "\n",
    "        # Word-level attention module\n",
    "        self.word_attention = WordAttention(vocab_size, embed_dim, word_gru_hidden_dim, word_gru_num_layers, word_att_dim, use_layer_norm, dropout)\n",
    "\n",
    "        # Bidirectional sentence-level GRU\n",
    "        self.gru = nn.GRU(2 * word_gru_hidden_dim, sent_gru_hidden_dim, num_layers=sent_gru_num_layers,\n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(2 * sent_gru_hidden_dim, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Sentence-level attention\n",
    "        self.sent_attention = nn.Linear(2 * sent_gru_hidden_dim, sent_att_dim)\n",
    "\n",
    "        # Sentence context vector u_s to take dot product with\n",
    "        # This is equivalent to taking that dot product (Eq.10 in the paper),\n",
    "        # as u_s is the linear layer's 1D parameter vector here\n",
    "        self.sentence_context_vector = nn.Linear(sent_att_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, code_tensor, code_lengths, sent_lengths):\n",
    "\n",
    "        # Sort code_tensor by decreasing order in length\n",
    "        code_lengths, code_perm_idx = code_lengths.sort(dim=0, descending=True)\n",
    "        code_tensor = code_tensor[code_perm_idx]\n",
    "        sent_lengths = sent_lengths[code_perm_idx]\n",
    "\n",
    "        # Make a long batch of sentences by removing pad-sentences\n",
    "        # i.e. `code_tensor` was of size (num_code_tensor, padded_code_lengths, padded_sent_length)\n",
    "        # -> `packed_sents.data` is now of size (num_sents, padded_sent_length)\n",
    "        packed_sents = pack_padded_sequence(code_tensor, lengths=code_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # effective batch size at each timestep\n",
    "        valid_bsz = packed_sents.batch_sizes\n",
    "\n",
    "        # Make a long batch of sentence lengths by removing pad-sentences\n",
    "        # i.e. `sent_lengths` was of size (num_code_tensor, padded_code_lengths)\n",
    "        # -> `packed_sent_lengths.data` is now of size (num_sents)\n",
    "        packed_sent_lengths = pack_padded_sequence(sent_lengths, lengths=code_lengths.tolist(), batch_first=True)\n",
    "\n",
    "    \n",
    "    \n",
    "        # Word attention module\n",
    "        sents, word_att_weights = self.word_attention(packed_sents.data, packed_sent_lengths.data)\n",
    "\n",
    "        sents = self.dropout(sents)\n",
    "\n",
    "        # Sentence-level GRU over sentence embeddings\n",
    "        packed_sents, _ = self.gru(PackedSequence(sents, valid_bsz))\n",
    "\n",
    "        if self.use_layer_norm:\n",
    "            normed_sents = self.layer_norm(packed_sents.data)\n",
    "        else:\n",
    "            normed_sents = packed_sents\n",
    "\n",
    "        # Sentence attention\n",
    "        att = torch.tanh(self.sent_attention(normed_sents))\n",
    "        att = self.sentence_context_vector(att).squeeze(1)\n",
    "\n",
    "        val = att.max()\n",
    "        att = torch.exp(att - val)\n",
    "\n",
    "        # Restore as documents by repadding\n",
    "        att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
    "\n",
    "        sent_att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
    "\n",
    "        # Restore as documents by repadding\n",
    "        code_tensor, _ = pad_packed_sequence(packed_sents, batch_first=True)\n",
    "\n",
    "        # Compute document vectors\n",
    "        code_tensor = code_tensor * sent_att_weights.unsqueeze(2)\n",
    "        code_tensor = code_tensor.sum(dim=1)\n",
    "\n",
    "        # Restore as documents by repadding\n",
    "        word_att_weights, _ = pad_packed_sequence(PackedSequence(word_att_weights, valid_bsz), batch_first=True)\n",
    "\n",
    "        # Restore the original order of documents (undo the first sorting)\n",
    "        _, code_tensor_unperm_idx = code_perm_idx.sort(dim=0, descending=False)\n",
    "        code_tensor = code_tensor[code_tensor_unperm_idx]\n",
    "\n",
    "        word_att_weights = word_att_weights[code_tensor_unperm_idx]\n",
    "        sent_att_weights = sent_att_weights[code_tensor_unperm_idx]\n",
    "\n",
    "        return code_tensor, word_att_weights, sent_att_weights, sents\n",
    "\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Word-level attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, gru_hidden_dim, gru_num_layers, att_dim, use_layer_norm, dropout):\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # output (batch, hidden_size)\n",
    "        self.gru = nn.GRU(embed_dim, gru_hidden_dim, num_layers=gru_num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(2 * gru_hidden_dim, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Maps gru output to `att_dim` sized tensor\n",
    "        self.attention = nn.Linear(2 * gru_hidden_dim, att_dim)\n",
    "\n",
    "        # Word context vector (u_w) to take dot-product with\n",
    "        self.context_vector = nn.Linear(att_dim, 1, bias=False)\n",
    "\n",
    "    def init_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Initialized embedding layer with pretrained embeddings.\n",
    "        embeddings: embeddings to init with\n",
    "        \"\"\"\n",
    "        self.embeddings.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def freeze_embeddings(self, freeze=False):\n",
    "        \"\"\"\n",
    "        Set whether to freeze pretrained embeddings.\n",
    "        \"\"\"\n",
    "        self.embeddings.weight.requires_grad = freeze\n",
    "\n",
    "    def forward(self, sents, sent_lengths):\n",
    "        \"\"\"\n",
    "        sents: encoded sentence-level data; LongTensor (num_sents, pad_len, embed_dim)\n",
    "        return: sentence embeddings, attention weights of words\n",
    "        \"\"\"\n",
    "        # Sort sents by decreasing order in sentence lengths\n",
    "        sent_lengths, sent_perm_idx = sent_lengths.sort(dim=0, descending=True)\n",
    "        sents = sents[sent_perm_idx]\n",
    "\n",
    "        sents = self.embeddings(sents)\n",
    "\n",
    "        packed_words = pack_padded_sequence(sents, lengths=sent_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # effective batch size at each timestep\n",
    "        valid_bsz = packed_words.batch_sizes\n",
    "\n",
    "        # Apply word-level GRU over word embeddings\n",
    "        packed_words, _ = self.gru(packed_words)\n",
    "\n",
    "        if self.use_layer_norm:\n",
    "            normed_words = self.layer_norm(packed_words.data)\n",
    "        else:\n",
    "            normed_words = packed_words\n",
    "\n",
    "        # Word Attenton\n",
    "        att = torch.tanh(self.attention(normed_words.data))\n",
    "        att = self.context_vector(att).squeeze(1)\n",
    "\n",
    "        val = att.max()\n",
    "        att = torch.exp(att - val) # att.size: (n_words)\n",
    "\n",
    "        # Restore as sentences by repadding\n",
    "        att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
    "\n",
    "        att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
    "\n",
    "        # Restore as sentences by repadding\n",
    "        sents, _ = pad_packed_sequence(packed_words, batch_first=True)\n",
    "\n",
    "        # Compute sentence vectors\n",
    "        sents = sents * att_weights.unsqueeze(2)\n",
    "        sents = sents.sum(dim=1)\n",
    "\n",
    "        # Restore the original order of sentences (undo the first sorting)\n",
    "        _, sent_unperm_idx = sent_perm_idx.sort(dim=0, descending=False)\n",
    "        sents = sents[sent_unperm_idx]\n",
    "\n",
    "        att_weights = att_weights[sent_unperm_idx]\n",
    "\n",
    "        return sents, att_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212cf17-7c43-4ee4-906b-26ba1b694220",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13cc30d-da19-4aa6-832c-87d17037997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# model setting\n",
    "batch_size = 32 #args.batch_size\n",
    "num_epochs = 10 #args.num_epochs\n",
    "max_grad_norm = 5\n",
    "embed_dim = 50 #args.embed_dim\n",
    "word_gru_hidden_dim = 64 #args.word_gru_hidden_dim\n",
    "sent_gru_hidden_dim = 64 #args.sent_gru_hidden_dim\n",
    "word_gru_num_layers = 2 #args.word_gru_num_layers\n",
    "sent_gru_num_layers = 2 #args.sent_gru_num_layers\n",
    "word_att_dim = 64\n",
    "sent_att_dim = 64\n",
    "use_layer_norm = True\n",
    "dropout = 0.1 #args.dropout\n",
    "lr = 0.001 #args.lr\n",
    "\n",
    "save_every_epochs = 1\n",
    "exp_name = ''#args.exp_name\n",
    "\n",
    "max_train_LOC = 900\n",
    "\n",
    "weight_dict = {}\n",
    "\n",
    "def get_loss_weight(labels):\n",
    "    '''\n",
    "        input\n",
    "            labels: a PyTorch tensor that contains labels\n",
    "        output\n",
    "            weight_tensor: a PyTorch tensor that contains weight of defect/clean class\n",
    "    '''\n",
    "    label_list = labels.cpu().numpy().squeeze().tolist()\n",
    "    weight_list = []\n",
    "\n",
    "    for lab in label_list:\n",
    "        if lab == 0:\n",
    "            weight_list.append(weight_dict['clean'])\n",
    "        else:\n",
    "            weight_list.append(weight_dict['defect'])\n",
    "\n",
    "    weight_tensor = torch.tensor(weight_list).reshape(-1,1)\n",
    "    return weight_tensor\n",
    "\n",
    "\n",
    "def train_model():\n",
    "\n",
    "\n",
    "    train_code3d, train_label = get_code3d_and_label(train_df)\n",
    "\n",
    "    sample_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_label), y = train_label)\n",
    "\n",
    "    weight_dict['defect'] = np.max(sample_weights)\n",
    "    weight_dict['clean'] = np.min(sample_weights)\n",
    "\n",
    "    word2vec = Word2Vec.load('./word2vec/w2v50dim.bin')\n",
    "    print('load Word2Vec finished')\n",
    "\n",
    "    word2vec_weights = get_w2v_weight_for_deep_learning_models(word2vec, embed_dim)\n",
    "\n",
    "    vocab_size = len(word2vec.wv.key_to_index) + 1  # Use key_to_index\n",
    "    # for unknown tokens\n",
    "\n",
    "    x_train_vec = get_x_vec(train_code3d, word2vec)\n",
    "    # x_valid_vec = get_x_vec(valid_code3d, word2vec)\n",
    "\n",
    "    max_sent_len = min(max([len(sent) for sent in (x_train_vec)]), max_train_LOC)\n",
    "\n",
    "    # print(x_train_vec[0])\n",
    "\n",
    "    train_dl = get_dataloader(x_train_vec,train_label,batch_size,max_sent_len)\n",
    "\n",
    "    # valid_dl = get_dataloader(x_valid_vec, valid_label,batch_size,max_sent_len)\n",
    "\n",
    "    model = HierarchicalAttentionNetwork(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        word_gru_hidden_dim=word_gru_hidden_dim,\n",
    "        sent_gru_hidden_dim=sent_gru_hidden_dim,\n",
    "        word_gru_num_layers=word_gru_num_layers,\n",
    "        sent_gru_num_layers=sent_gru_num_layers,\n",
    "        word_att_dim=word_att_dim,\n",
    "        sent_att_dim=sent_att_dim,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        dropout=dropout)\n",
    "\n",
    "    # model = model.cuda()\n",
    "    model.sent_attention.word_attention.freeze_embeddings(False)\n",
    "    \n",
    "    # print some word attention weights\n",
    "    for inputs, labels in train_dl:\n",
    "        output, word_att_weights, sent_att_weights, sents = model(inputs)\n",
    "        print(word_att_weights)\n",
    "        break\n",
    "    \n",
    "\n",
    "    optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "\n",
    "            # # inputs_cuda, labels_cuda = inputs.cuda(), labels.cuda()\n",
    "            # output, _, __, ___ = model(inputs)\n",
    "            # i need outputs and attention weights from model\n",
    "            output,word_att_weights,sent_att_weights,sents = model(inputs)\n",
    "\n",
    "            weight_tensor = get_loss_weight(labels)\n",
    "\n",
    "            criterion.weight = weight_tensor\n",
    "\n",
    "            loss = criterion(output, labels.reshape(batch_size,1))\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch}: {time.time()-start}')\n",
    "\n",
    "    torch.save(model.state_dict(), './DeepLineDPReplication.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e3d83b-f314-4b33-b60f-fe72e490281f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Word2Vec finished\n",
      "code_vec shape: 150\n",
      "Y_tensor shape: 150\n",
      "code_vec_pad shape: 150\n",
      "tensor([[[0.0136, 0.0174, 0.0138,  ..., 0.0237, 0.0219, 0.0249],\n",
      "         [0.0211, 0.0208, 0.0188,  ..., 0.0231, 0.0219, 0.0206],\n",
      "         [0.0203, 0.0185, 0.0163,  ..., 0.0223, 0.0238, 0.0254],\n",
      "         ...,\n",
      "         [0.0215, 0.0236, 0.0186,  ..., 0.0211, 0.0221, 0.0227],\n",
      "         [0.0229, 0.0207, 0.0185,  ..., 0.0203, 0.0212, 0.0213],\n",
      "         [0.0205, 0.0222, 0.0191,  ..., 0.0220, 0.0218, 0.0250]],\n",
      "\n",
      "        [[0.0166, 0.0195, 0.0155,  ..., 0.0233, 0.0234, 0.0262],\n",
      "         [0.0198, 0.0232, 0.0197,  ..., 0.0207, 0.0234, 0.0236],\n",
      "         [0.0201, 0.0200, 0.0176,  ..., 0.0209, 0.0230, 0.0216],\n",
      "         ...,\n",
      "         [0.0194, 0.0184, 0.0171,  ..., 0.0207, 0.0214, 0.0251],\n",
      "         [0.0202, 0.0196, 0.0166,  ..., 0.0237, 0.0272, 0.0241],\n",
      "         [0.0217, 0.0189, 0.0183,  ..., 0.0211, 0.0221, 0.0227]],\n",
      "\n",
      "        [[0.0147, 0.0188, 0.0166,  ..., 0.0221, 0.0251, 0.0278],\n",
      "         [0.0220, 0.0198, 0.0177,  ..., 0.0203, 0.0205, 0.0222],\n",
      "         [0.0206, 0.0199, 0.0169,  ..., 0.0237, 0.0227, 0.0240],\n",
      "         ...,\n",
      "         [0.0201, 0.0178, 0.0164,  ..., 0.0213, 0.0220, 0.0222],\n",
      "         [0.0229, 0.0207, 0.0174,  ..., 0.0213, 0.0227, 0.0230],\n",
      "         [0.0213, 0.0202, 0.0170,  ..., 0.0214, 0.0221, 0.0237]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0150, 0.0173, 0.0150,  ..., 0.0221, 0.0231, 0.0266],\n",
      "         [0.0213, 0.0190, 0.0180,  ..., 0.0227, 0.0227, 0.0236],\n",
      "         [0.0219, 0.0202, 0.0164,  ..., 0.0209, 0.0207, 0.0231],\n",
      "         ...,\n",
      "         [0.0215, 0.0212, 0.0184,  ..., 0.0218, 0.0226, 0.0237],\n",
      "         [0.0200, 0.0193, 0.0173,  ..., 0.0203, 0.0211, 0.0222],\n",
      "         [0.0208, 0.0201, 0.0189,  ..., 0.0201, 0.0215, 0.0231]],\n",
      "\n",
      "        [[0.0155, 0.0185, 0.0160,  ..., 0.0205, 0.0220, 0.0229],\n",
      "         [0.0231, 0.0230, 0.0199,  ..., 0.0224, 0.0226, 0.0237],\n",
      "         [0.0211, 0.0221, 0.0172,  ..., 0.0212, 0.0209, 0.0210],\n",
      "         ...,\n",
      "         [0.0209, 0.0199, 0.0180,  ..., 0.0205, 0.0234, 0.0239],\n",
      "         [0.0208, 0.0205, 0.0180,  ..., 0.0220, 0.0235, 0.0257],\n",
      "         [0.0206, 0.0222, 0.0200,  ..., 0.0208, 0.0213, 0.0235]],\n",
      "\n",
      "        [[0.0162, 0.0186, 0.0158,  ..., 0.0213, 0.0225, 0.0235],\n",
      "         [0.0171, 0.0167, 0.0156,  ..., 0.0215, 0.0231, 0.0259],\n",
      "         [0.0158, 0.0146, 0.0177,  ..., 0.0229, 0.0222, 0.0240],\n",
      "         ...,\n",
      "         [0.0189, 0.0197, 0.0172,  ..., 0.0211, 0.0241, 0.0238],\n",
      "         [0.0213, 0.0230, 0.0200,  ..., 0.0209, 0.0218, 0.0235],\n",
      "         [0.0211, 0.0203, 0.0177,  ..., 0.0203, 0.0204, 0.0243]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Epoch 0: 10.935353517532349\n",
      "Epoch 1: 10.898205041885376\n",
      "Epoch 2: 10.411277532577515\n",
      "Epoch 3: 10.120939016342163\n",
      "Epoch 4: 10.073407649993896\n",
      "Epoch 5: 10.20724892616272\n",
      "Epoch 6: 10.92986512184143\n",
      "Epoch 7: 10.614256858825684\n",
      "Epoch 8: 10.555871486663818\n",
      "Epoch 9: 10.327548742294312\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2210344-f737-40dd-8ea4-93a88b68b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(test_df)\n",
    "torch.manual_seed(0)\n",
    "embed_dim = 50 #args.embed_dim\n",
    "word_gru_hidden_dim = 64 #args.word_gru_hidden_dim\n",
    "sent_gru_hidden_dim = 64 #args.sent_gru_hidden_dim\n",
    "word_gru_num_layers = 2 #args.word_gru_num_layers\n",
    "sent_gru_num_layers = 2 #args.sent_gru_num_layers\n",
    "word_att_dim = 64\n",
    "sent_att_dim = 64\n",
    "use_layer_norm = True\n",
    "dropout = 0.1 #args.dropout\n",
    "lr = 0.001 #args.lr\n",
    "max_test_LOC = 900\n",
    "\n",
    "def test_model():\n",
    "\n",
    "\n",
    "    test_code3d, test_label = get_code3d_and_label(test_df)\n",
    "\n",
    "    sample_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(test_label), y = test_label)\n",
    "\n",
    "    weight_dict['defect'] = np.max(sample_weights)\n",
    "    weight_dict['clean'] = np.min(sample_weights)\n",
    "\n",
    "    word2vec = Word2Vec.load('./word2vec/w2v50dim.bin')\n",
    "    print('load Word2Vec finished')\n",
    "\n",
    "    word2vec_weights = get_w2v_weight_for_deep_learning_models(word2vec, embed_dim)\n",
    "\n",
    "    vocab_size = len(word2vec.wv.key_to_index) + 1  # Use key_to_index\n",
    "    # for unknown tokens\n",
    "\n",
    "    x_test_vec = get_x_vec(test_code3d, word2vec)\n",
    "\n",
    "    max_sent_len = min(max([len(sent) for sent in (x_test_vec)]), max_test_LOC)\n",
    "\n",
    "    test_dl = get_dataloader(x_test_vec,test_label,batch_size,max_sent_len)\n",
    "\n",
    "    loaded_dict = torch.load('./DeepLineDPReplication.pth')\n",
    "    model = HierarchicalAttentionNetwork(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        word_gru_hidden_dim=word_gru_hidden_dim,\n",
    "        sent_gru_hidden_dim=sent_gru_hidden_dim,\n",
    "        word_gru_num_layers=word_gru_num_layers,\n",
    "        sent_gru_num_layers=sent_gru_num_layers,\n",
    "        word_att_dim=word_att_dim,\n",
    "        sent_att_dim=sent_att_dim,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    model.load_state_dict(loaded_dict)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in test_dl:\n",
    "    #     outputs, _, __, ___ = model(inputs)\n",
    "        outputs, word_att_weights, sent_att_weights, sents = model(inputs)\n",
    "        return outputs,word_att_weights,sent_att_weights, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5f465cd-8e70-451a-ac76-17c3809b03d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Word2Vec finished\n",
      "code_vec shape: 150\n",
      "Y_tensor shape: 150\n",
      "code_vec_pad shape: 150\n"
     ]
    }
   ],
   "source": [
    "y_pred,word_att_weights,sent_att_weights, y_gt = test_model() # test --> changed to train\n",
    "# y_pred, y_gt = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a8878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2644, 0.0440, 0.0272,  ..., 0.0113, 0.0107, 0.0104],\n",
      "        [0.5365, 0.0208, 0.0137,  ..., 0.0074, 0.0070, 0.0068],\n",
      "        [0.5585, 0.0212, 0.0136,  ..., 0.0070, 0.0066, 0.0064],\n",
      "        ...,\n",
      "        [0.5578, 0.0783, 0.0183,  ..., 0.0059, 0.0055, 0.0054],\n",
      "        [0.6976, 0.1461, 0.0348,  ..., 0.0015, 0.0014, 0.0014],\n",
      "        [0.5365, 0.0208, 0.0137,  ..., 0.0074, 0.0070, 0.0068]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sent_att_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33d31947-6c9e-47df-9cb8-2cc47895bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.detach().numpy()\n",
    "word_att_weights = word_att_weights.detach().numpy()\n",
    "sent_att_weights = sent_att_weights.detach().numpy()\n",
    "y_gt = y_gt.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1e8d66c-16f0-4392-bacb-511abdf6e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = np.array([prob[0] for prob in y_pred])\n",
    "word_att_weights = np.array([att[0] for att in word_att_weights])\n",
    "sent_att_weights = np.array([att[0] for att in sent_att_weights])\n",
    "y_pred = np.where(y_probs >= 0.45, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5ffb3f6-d604-4d5f-b466-722fcf47fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "149b162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e477e6a-062d-4114-8de3-3ad276db5993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.49\n",
      "Recall: 1.0\n",
      "F1-score: 0.6577181208053692\n",
      "False Alarm Rate: 1.0\n",
      "Distance to Heaven: 0.7071067811865476\n",
      "AUC:0.49279711884753896\n"
     ]
    }
   ],
   "source": [
    "# precision, Recall, F1-score, Confusion matrix, False Alarm Rate, Distance-to-Heaven, AUC\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "\n",
    "prec, rec, f1, _ = metrics.precision_recall_fscore_support(y_gt,y_pred,average='binary') # at threshold = 0.5\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_gt, y_pred, labels=[0, 1]).ravel()\n",
    "FAR = fp/(fp+tn)\n",
    "dist_heaven = math.sqrt((pow(1-rec,2)+pow(0-FAR,2))/2.0)\n",
    "AUC = metrics.roc_auc_score(y_gt, y_probs)\n",
    "\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"False Alarm Rate: {FAR}\")\n",
    "print(f\"Distance to Heaven: {dist_heaven}\")\n",
    "print(f\"AUC:{AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33a18e72-472b-4679-a3dc-71d2d79c2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./toscore_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "457bfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6671cb5-61b4-4d1a-9166-8f03e1bc8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'commit', 'repo', 'filepath', 'content', 'methods', 'lines',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "599b4918-5c20-47a2-b0cf-5330e4580e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save.fig.dir\n",
    "save_fig_dir = '../output/figure/'\n",
    "os.makedirs(save_fig_dir, exist_ok=True)\n",
    "\n",
    "def preprocess(x, reverse):\n",
    "    x.columns = [\"variable\",\"value\"]\n",
    "    tmp = pd.concat([x[\"variable\"], x[\"value\"]], axis=1)\n",
    "    tmp = tmp.pivot(columns=\"variable\", values=\"value\")\n",
    "    tmp.columns = tmp.columns.str.replace(\".value\", \"\")\n",
    "    df = tmp\n",
    "    ranking = None\n",
    "    \n",
    "    if reverse == True:\n",
    "        ranking = (max(df.columns) - df.columns) + 1\n",
    "    else:\n",
    "        ranking = df.columns\n",
    "    \n",
    "    df[\"rank\"] = \"Rank\" + ranking.astype(str)\n",
    "    return df\n",
    "\n",
    "def get_top_k_tokens(df, k):\n",
    "    top_k = df[(df[\"is.comment.line\"] == \"False\") & (df[\"file.level.ground.truth\"] == \"True\") & (df[\"prediction.label\"] == \"True\")]\n",
    "    top_k = top_k.groupby([\"test\", \"filename\"]).apply(lambda x: x.nlargest(k, \"token.attention.score\")).reset_index(drop=True)\n",
    "    top_k = top_k[[\"project\", \"train\", \"test\", \"filename\", \"token\"]].drop_duplicates()\n",
    "    top_k[\"flag\"] = \"topk\"\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e8e667-bacd-4a3a-9abb-4fa66283c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prediction_dir = '../output/prediction/DeepLineDP/within-release/'\n",
    "# all_files = os.listdir(prediction_dir)\n",
    "\n",
    "# df_all = pd.DataFrame()\n",
    "\n",
    "# for f in all_files:\n",
    "#     df = pd.read_csv(os.path.join(prediction_dir, f))\n",
    "#     df_all = pd.concat([df_all, df])\n",
    "df_all = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc4feae2-0ea0-4ab2-ac8a-a3e35afa8cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>content</th>\n",
       "      <th>methods</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-13 15:35:10+04:00</td>\n",
       "      <td>000fbe63d390c59b9c1e29216c35fc52b991f2f3</td>\n",
       "      <td>lightning</td>\n",
       "      <td>pytorch_lightning\\trainer\\connectors\\logger_co...</td>\n",
       "      <td>b'# Copyright The PyTorch Lightning team.\\n#\\n...</td>\n",
       "      <td>[extract_batch_size, _extract_batch_size]</td>\n",
       "      <td>[17, 24, 593]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-11 13:12:55+04:00</td>\n",
       "      <td>038d5338530411bb47283fda1e84dec91137880b</td>\n",
       "      <td>localstack</td>\n",
       "      <td>localstack\\aws\\app.py</td>\n",
       "      <td>b'import logging\\n\\nfrom localstack.aws import...</td>\n",
       "      <td>[__init__]</td>\n",
       "      <td>[62]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-16 08:51:12+04:00</td>\n",
       "      <td>0786e84a33155ebc8d8d3502e3a7f3060b86a4ec</td>\n",
       "      <td>scrapy</td>\n",
       "      <td>scrapy\\utils\\iterators.py</td>\n",
       "      <td>b'import re, csv, six\\n\\ntry:\\n    from cStrin...</td>\n",
       "      <td>[csviter]</td>\n",
       "      <td>[3, 4, 5, 6, 7, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-28 17:22:24+04:00</td>\n",
       "      <td>0094cb0d0472b08f92915e948907b237eea020e3</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>spacy\\cli\\train.py</td>\n",
       "      <td>b'from typing import Optional, Dict, Any, Tupl...</td>\n",
       "      <td>[update_meta]</td>\n",
       "      <td>[449]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-18 21:30:15+04:00</td>\n",
       "      <td>03c6f4bf250edd18eb818ed65090f508636b0bff</td>\n",
       "      <td>localstack</td>\n",
       "      <td>localstack\\services\\awslambda\\lambda_executors.py</td>\n",
       "      <td>b'import os\\nimport re\\nimport glob\\nimport js...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[30, 31, 33, 37, 39]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                    commit  \\\n",
       "0 2021-07-13 15:35:10+04:00  000fbe63d390c59b9c1e29216c35fc52b991f2f3   \n",
       "1 2022-07-11 13:12:55+04:00  038d5338530411bb47283fda1e84dec91137880b   \n",
       "2 2014-07-16 08:51:12+04:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
       "3 2020-07-28 17:22:24+04:00  0094cb0d0472b08f92915e948907b237eea020e3   \n",
       "4 2020-01-18 21:30:15+04:00  03c6f4bf250edd18eb818ed65090f508636b0bff   \n",
       "\n",
       "         repo                                           filepath  \\\n",
       "0   lightning  pytorch_lightning\\trainer\\connectors\\logger_co...   \n",
       "1  localstack                              localstack\\aws\\app.py   \n",
       "2      scrapy                          scrapy\\utils\\iterators.py   \n",
       "3       spaCy                                 spacy\\cli\\train.py   \n",
       "4  localstack  localstack\\services\\awslambda\\lambda_executors.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  b'# Copyright The PyTorch Lightning team.\\n#\\n...   \n",
       "1  b'import logging\\n\\nfrom localstack.aws import...   \n",
       "2  b'import re, csv, six\\n\\ntry:\\n    from cStrin...   \n",
       "3  b'from typing import Optional, Dict, Any, Tupl...   \n",
       "4  b'import os\\nimport re\\nimport glob\\nimport js...   \n",
       "\n",
       "                                     methods                 lines  \n",
       "0  [extract_batch_size, _extract_batch_size]         [17, 24, 593]  \n",
       "1                                 [__init__]                  [62]  \n",
       "2                                  [csviter]   [3, 4, 5, 6, 7, 55]  \n",
       "3                              [update_meta]                 [449]  \n",
       "4                                         []  [30, 31, 33, 37, 39]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_deep_random = '.'\n",
    "test = pd.read_parquet(f'{path_to_deep_random}/test.parquet.gzip')\n",
    "test = test.reset_index(drop=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "713ce99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"./init_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b050839b-811e-434e-985f-331997bdab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17  24 593]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x=test['content'][0]\n",
    "\n",
    "def process_byte_column(df, i):\n",
    "    text_data = df['content'].iloc[i].decode('latin-1', errors='replace')\n",
    "    lines = text_data.split('\\n')\n",
    "    return lines\n",
    "print(test['lines'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "229b41a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9261ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column which is a array label for comment lines\n",
    "is_comment = False\n",
    "columns = ['datetime', 'commit', 'repo', 'filepath', 'is_comment', 'line', 'is_buggy','attention_score']\n",
    "tdf = pd.DataFrame(columns=columns)\n",
    "# tdf.columns = ['datetime', 'commit', 'repo', 'filepath', 'is_comment', 'line', 'is_buggy']\n",
    "row_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c647bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'commit', 'repo', 'filepath', 'is_comment', 'line',\n",
      "       'is_buggy', 'attention_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(tdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb4ce461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0124259265\n"
     ]
    }
   ],
   "source": [
    "print((word_att_weights[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0aeaa101-842d-4bc3-a722-37bf8992ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "doc num:1\n",
      "doc num:2\n",
      "doc num:3\n",
      "doc num:4\n",
      "doc num:5\n",
      "doc num:6\n",
      "doc num:7\n",
      "doc num:8\n",
      "doc num:9\n",
      "doc num:10\n",
      "doc num:11\n",
      "doc num:12\n",
      "doc num:13\n",
      "doc num:14\n",
      "doc num:15\n",
      "doc num:16\n",
      "doc num:17\n",
      "doc num:18\n",
      "doc num:19\n",
      "doc num:20\n",
      "doc num:21\n",
      "doc num:22\n",
      "doc num:23\n",
      "doc num:24\n",
      "doc num:25\n",
      "doc num:26\n",
      "doc num:27\n",
      "doc num:28\n",
      "doc num:29\n",
      "doc num:30\n",
      "doc num:31\n",
      "doc num:32\n",
      "doc num:33\n",
      "doc num:34\n",
      "doc num:35\n",
      "doc num:36\n",
      "doc num:37\n",
      "doc num:38\n",
      "doc num:39\n",
      "doc num:40\n",
      "doc num:41\n",
      "doc num:42\n",
      "doc num:43\n",
      "doc num:44\n",
      "doc num:45\n",
      "doc num:46\n",
      "doc num:47\n",
      "doc num:48\n",
      "doc num:49\n",
      "doc num:50\n",
      "doc num:51\n",
      "doc num:52\n",
      "doc num:53\n",
      "doc num:54\n",
      "doc num:55\n",
      "doc num:56\n",
      "doc num:57\n",
      "doc num:58\n",
      "doc num:59\n",
      "doc num:60\n",
      "doc num:61\n",
      "doc num:62\n",
      "doc num:63\n",
      "doc num:64\n",
      "doc num:65\n",
      "doc num:66\n",
      "doc num:67\n",
      "doc num:68\n",
      "doc num:69\n",
      "doc num:70\n",
      "doc num:71\n",
      "doc num:72\n",
      "doc num:73\n",
      "doc num:74\n",
      "doc num:75\n",
      "doc num:76\n",
      "doc num:77\n",
      "doc num:78\n",
      "doc num:79\n",
      "doc num:80\n",
      "doc num:81\n",
      "doc num:82\n",
      "doc num:83\n",
      "doc num:84\n",
      "doc num:85\n",
      "doc num:86\n",
      "doc num:87\n",
      "doc num:88\n",
      "doc num:89\n",
      "doc num:90\n",
      "doc num:91\n",
      "doc num:92\n",
      "doc num:93\n",
      "doc num:94\n",
      "doc num:95\n",
      "doc num:96\n",
      "doc num:97\n",
      "doc num:98\n",
      "doc num:99\n",
      "doc num:101\n",
      "doc num:102\n",
      "doc num:103\n",
      "doc num:104\n",
      "doc num:105\n",
      "doc num:106\n",
      "doc num:107\n",
      "doc num:108\n",
      "doc num:109\n",
      "doc num:110\n",
      "doc num:111\n",
      "doc num:112\n",
      "doc num:113\n",
      "doc num:114\n",
      "doc num:115\n",
      "doc num:116\n",
      "doc num:117\n",
      "doc num:118\n",
      "doc num:119\n",
      "doc num:120\n",
      "doc num:121\n",
      "doc num:122\n",
      "doc num:123\n",
      "doc num:124\n",
      "doc num:125\n",
      "doc num:126\n",
      "doc num:127\n",
      "doc num:128\n",
      "doc num:129\n",
      "doc num:130\n",
      "doc num:131\n",
      "doc num:132\n",
      "doc num:133\n",
      "doc num:134\n",
      "doc num:135\n",
      "doc num:136\n",
      "doc num:137\n",
      "doc num:138\n",
      "doc num:139\n",
      "doc num:140\n",
      "doc num:141\n",
      "doc num:142\n",
      "doc num:143\n",
      "doc num:144\n",
      "doc num:145\n",
      "doc num:146\n",
      "doc num:147\n",
      "doc num:148\n",
      "doc num:149\n",
      "doc num:150\n",
      "doc num:151\n",
      "doc num:152\n",
      "doc num:153\n",
      "doc num:154\n",
      "doc num:155\n",
      "doc num:156\n",
      "doc num:157\n",
      "doc num:158\n",
      "doc num:159\n",
      "doc num:160\n",
      "doc num:161\n",
      "doc num:162\n",
      "doc num:163\n",
      "doc num:164\n",
      "doc num:165\n",
      "doc num:166\n",
      "doc num:167\n",
      "doc num:168\n",
      "doc num:169\n",
      "doc num:170\n",
      "doc num:171\n",
      "doc num:172\n",
      "doc num:173\n",
      "doc num:174\n",
      "doc num:175\n",
      "doc num:176\n",
      "doc num:177\n",
      "doc num:178\n",
      "doc num:179\n",
      "doc num:180\n",
      "doc num:181\n",
      "doc num:182\n",
      "doc num:183\n",
      "doc num:184\n",
      "doc num:185\n",
      "doc num:186\n",
      "doc num:187\n",
      "doc num:188\n",
      "doc num:189\n",
      "doc num:190\n",
      "doc num:191\n",
      "doc num:192\n",
      "doc num:193\n",
      "doc num:194\n",
      "doc num:195\n",
      "doc num:196\n",
      "doc num:197\n",
      "doc num:198\n",
      "doc num:199\n",
      "doc num:201\n",
      "doc num:202\n",
      "doc num:203\n",
      "doc num:204\n",
      "doc num:205\n",
      "doc num:206\n",
      "doc num:207\n",
      "doc num:208\n",
      "doc num:209\n",
      "doc num:210\n",
      "doc num:211\n",
      "doc num:212\n",
      "doc num:213\n",
      "doc num:214\n",
      "doc num:215\n",
      "doc num:216\n",
      "doc num:217\n",
      "doc num:218\n",
      "doc num:219\n",
      "doc num:220\n",
      "doc num:221\n",
      "doc num:222\n",
      "doc num:223\n",
      "doc num:224\n",
      "doc num:225\n",
      "doc num:226\n",
      "doc num:227\n",
      "doc num:228\n",
      "doc num:229\n",
      "doc num:230\n",
      "doc num:231\n",
      "doc num:232\n",
      "doc num:233\n",
      "doc num:234\n",
      "doc num:235\n",
      "doc num:236\n",
      "doc num:237\n",
      "doc num:238\n",
      "doc num:239\n",
      "doc num:240\n",
      "doc num:241\n",
      "doc num:242\n",
      "doc num:243\n",
      "doc num:244\n",
      "doc num:245\n",
      "doc num:246\n",
      "doc num:247\n",
      "doc num:248\n",
      "doc num:249\n",
      "doc num:250\n",
      "doc num:251\n",
      "doc num:252\n",
      "doc num:253\n",
      "doc num:254\n",
      "doc num:255\n",
      "doc num:256\n",
      "doc num:257\n",
      "doc num:258\n",
      "doc num:259\n",
      "doc num:260\n",
      "doc num:261\n",
      "doc num:262\n",
      "doc num:263\n",
      "doc num:264\n",
      "doc num:265\n",
      "doc num:266\n",
      "doc num:267\n",
      "doc num:268\n",
      "doc num:269\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(len(train)):\n",
    "print(len(test))\n",
    "for i in range(len(test)):\n",
    "    temp = process_byte_column(test, i)\n",
    "    j = 0  # line number in the file\n",
    "    if i % 100:\n",
    "        print(f'doc num:{i}')\n",
    "    is_comment = False\n",
    "    for line in temp:\n",
    "        j += 1\n",
    "        row_dict = []\n",
    "        row_dict.append(test['datetime'][i])\n",
    "        row_dict.append(test['commit'][i])\n",
    "        row_dict.append(test['repo'][i])\n",
    "        row_dict.append(test['filepath'][i])\n",
    "        if line.startswith('#'):\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif line.startswith(\"'''\"):\n",
    "            is_comment = True\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif is_comment:\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif line.endswith(\"'''\") and is_comment:\n",
    "            is_comment = False\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif line == '':\n",
    "            continue\n",
    "        else:\n",
    "            row_dict.append(False)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False if j not in test['lines'][i] else True)\n",
    "        # tdf[attention_score]. Each word score must be updated from word attention weights in the test model output\n",
    "        if j < len(word_att_weights):\n",
    "            row_dict.append([word_att_weights[j][k]\n",
    "                            for k in range(len(word_att_weights[j]))])\n",
    "        else:\n",
    "            row_dict.append(None)  # or some other default value\n",
    "        tdf.loc[len(tdf.index)] = row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887de08-50a1-49a2-a2b7-408815c13e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tdf['is_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93507f9d-e00b-4a47-aa21-6535f99f00f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tdf.to_csv(\"./line_lvl_eval.csv\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load line_lvl_eval.csv to tdf dataframe\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m tdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tdf_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# tdf.to_csv(\"./line_lvl_eval.csv\")\n",
    "# load line_lvl_eval.csv to tdf dataframe\n",
    "tdf = pd.read_csv(\"./tdf_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a418e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>is_comment</th>\n",
       "      <th>line</th>\n",
       "      <th>is_buggy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-11 14:07:40-01:00</td>\n",
       "      <td>01b498ec5109da22bf1b79d86efaecf45426ad51</td>\n",
       "      <td>django-rest-framework</td>\n",
       "      <td>rest_framework\\schemas.py</td>\n",
       "      <td>False</td>\n",
       "      <td>from importlib import import_module</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-08-11 14:07:40-01:00</td>\n",
       "      <td>01b498ec5109da22bf1b79d86efaecf45426ad51</td>\n",
       "      <td>django-rest-framework</td>\n",
       "      <td>rest_framework\\schemas.py</td>\n",
       "      <td>False</td>\n",
       "      <td>from django.conf import settings</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-08-11 14:07:40-01:00</td>\n",
       "      <td>01b498ec5109da22bf1b79d86efaecf45426ad51</td>\n",
       "      <td>django-rest-framework</td>\n",
       "      <td>rest_framework\\schemas.py</td>\n",
       "      <td>False</td>\n",
       "      <td>from django.contrib.admindocs.views import sim...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-08-11 14:07:40-01:00</td>\n",
       "      <td>01b498ec5109da22bf1b79d86efaecf45426ad51</td>\n",
       "      <td>django-rest-framework</td>\n",
       "      <td>rest_framework\\schemas.py</td>\n",
       "      <td>False</td>\n",
       "      <td>from django.core.urlresolvers import RegexURLP...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-08-11 14:07:40-01:00</td>\n",
       "      <td>01b498ec5109da22bf1b79d86efaecf45426ad51</td>\n",
       "      <td>django-rest-framework</td>\n",
       "      <td>rest_framework\\schemas.py</td>\n",
       "      <td>False</td>\n",
       "      <td>from django.utils import six</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   datetime  \\\n",
       "0           0  2016-08-11 14:07:40-01:00   \n",
       "1           1  2016-08-11 14:07:40-01:00   \n",
       "2           2  2016-08-11 14:07:40-01:00   \n",
       "3           3  2016-08-11 14:07:40-01:00   \n",
       "4           4  2016-08-11 14:07:40-01:00   \n",
       "\n",
       "                                     commit                   repo  \\\n",
       "0  01b498ec5109da22bf1b79d86efaecf45426ad51  django-rest-framework   \n",
       "1  01b498ec5109da22bf1b79d86efaecf45426ad51  django-rest-framework   \n",
       "2  01b498ec5109da22bf1b79d86efaecf45426ad51  django-rest-framework   \n",
       "3  01b498ec5109da22bf1b79d86efaecf45426ad51  django-rest-framework   \n",
       "4  01b498ec5109da22bf1b79d86efaecf45426ad51  django-rest-framework   \n",
       "\n",
       "                    filepath  is_comment  \\\n",
       "0  rest_framework\\schemas.py       False   \n",
       "1  rest_framework\\schemas.py       False   \n",
       "2  rest_framework\\schemas.py       False   \n",
       "3  rest_framework\\schemas.py       False   \n",
       "4  rest_framework\\schemas.py       False   \n",
       "\n",
       "                                                line  is_buggy  \n",
       "0                from importlib import import_module     False  \n",
       "1                   from django.conf import settings     False  \n",
       "2  from django.contrib.admindocs.views import sim...     False  \n",
       "3  from django.core.urlresolvers import RegexURLP...     False  \n",
       "4                       from django.utils import six     False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "# Set save.fig.dir\n",
    "save_fig_dir = './output/figure/'\n",
    "os.makedirs(save_fig_dir, exist_ok=True)\n",
    "\n",
    "def preprocess(x, reverse):\n",
    "    x.columns = [\"variable\",\"value\"]\n",
    "    tmp = pd.concat([x[\"variable\"], x[\"value\"]], axis=1)\n",
    "    tmp = tmp.pivot(columns=\"variable\", values=\"value\")\n",
    "    tmp.columns = tmp.columns.str.replace(\".value\", \"\")\n",
    "    df = tmp\n",
    "    ranking = None\n",
    "    \n",
    "    if reverse == True:\n",
    "        ranking = (max(df.columns) - df.columns) + 1\n",
    "    else:\n",
    "        ranking = df.columns\n",
    "    \n",
    "    df[\"rank\"] = \"Rank\" + ranking.astype(str)\n",
    "    return df\n",
    "\n",
    "def get_top_k_tokens(df, k):\n",
    "    top_k = df[(df[\"is_comment\"] == \"False\") & (df[\"target\"] == \"True\") & (df[\"is_buggy\"] == \"True\")]\n",
    "    top_k = top_k.groupby([\"test\", \"filename\"]).apply(lambda x: x.nlargest(k, \"token.attention.score\")).reset_index(drop=True)\n",
    "    top_k = top_k[[\"project\", \"train\", \"test\", \"filename\", \"token\"]].drop_duplicates()\n",
    "    top_k[\"flag\"] = \"topk\"\n",
    "    return top_k\n",
    "\n",
    "prediction_dir = '../output/prediction/DeepLineDP/within-release/'\n",
    "all_files = os.listdir(prediction_dir)\n",
    "\n",
    "df_all = pd.DataFrame() \n",
    "\n",
    "for f in all_files:\n",
    "    df = pd.read_csv(os.path.join(prediction_dir, f))\n",
    "    df_all = pd.concat([df_all, df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Code for RQ1 -----------------------#\n",
    "\n",
    "##df_all will be tdf dataframe\n",
    "\n",
    "# RQ1-1\n",
    "df_to_plot = df_all[(df_all[\"is_comment\"] == \"False\") & (df_all[\"target\"] == \"True\") & (df_all[\"y_pred\"] == \"True\")]\n",
    "# df_to_plot = df_to_plot.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": [\"max\", \"min\", \"sd\"]}).reset_index()\n",
    "df_to_plot = df_to_plot.groupby([\"datetime\",\"commit\", \"repo\", \"filepath\",]).agg({\"token.attention.score\": [\"max\", \"min\", \"std\"]}).reset_index()\n",
    "df_to_plot.columns = [\"datetime\",\"commit\", \"repo\", \"filepath\", \"max\", \"min\", \"sd\"]\n",
    "# df_to_plot.columns = [\"test\", \"filename\", \"token\", \"Range\", \"SD\"]\n",
    "\n",
    "# RQ1-2\n",
    "df_all_copy = df_all.copy()\n",
    "df_all_copy = df_all_copy[(df_all_copy[\"is_comment\"] == \"False\") & (df_all_copy[\"target\"] == \"True\") & (df_all_copy[\"y_pred\"] == \"True\")]\n",
    "\n",
    "clean_lines_df = df_all_copy[df_all_copy[\"is_buggy\"] == \"False\"]\n",
    "buggy_lines_df = df_all_copy[df_all_copy[\"is_buggy\"] == \"True\"]\n",
    "\n",
    "# clean_lines_token_score = clean_lines_df.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": \"min\"}).reset_index()\n",
    "clean_lines_token_score = clean_lines_df.groupby([\"commit\", \"repo\", \"filepath\"]).agg({\"token.attention.score\": \"min\"}).reset_index()\n",
    "clean_lines_token_score[\"class\"] = \"Clean Lines\"\n",
    "\n",
    "# buggy_lines_token_score = buggy_lines_df.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": \"max\"}).reset_index()\n",
    "buggy_lines_token_score = buggy_lines_df.groupby([\"commit\", \"repo\", \"filepath\"]).agg({\"token.attention.score\": \"max\"}).reset_index()\n",
    "buggy_lines_token_score[\"class\"] = \"Defective Lines\"\n",
    "\n",
    "all_lines_token_score = pd.concat([buggy_lines_token_score, clean_lines_token_score])\n",
    "all_lines_token_score[\"class\"] = pd.Categorical(all_lines_token_score[\"class\"], categories=[\"Defective Lines\", \"Clean Lines\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_level_metrics(df_file):\n",
    "    all_gt = df_file[\"target\"]\n",
    "    all_prob = df_file[\"y_pred\"] #prediction.prob\n",
    "    all_pred = df_file[\"target\"] #prediction.label\n",
    "    \n",
    "    confusion_mat = confusion_matrix(all_pred, all_gt)\n",
    "    \n",
    "    bal_acc = confusion_mat[1, 1] / (confusion_mat[1, 1] + confusion_mat[0, 1])\n",
    "    AUC = roc_auc_score(all_gt, all_prob)\n",
    "    \n",
    "    all_pred = np.where(all_pred == \"False\", 0, 1)\n",
    "    all_gt = np.where(all_gt == \"False\", 0, 1)\n",
    "    \n",
    "    MCC = matthews_corrcoef(all_gt, all_pred)\n",
    "    \n",
    "    if np.isnan(MCC):\n",
    "        MCC = 0\n",
    "    \n",
    "    eval_result = [AUC, MCC, bal_acc]\n",
    "    \n",
    "    return eval_result\n",
    "\n",
    "def get_file_level_eval_result(prediction_dir, method_name):\n",
    "    all_files = os.listdir(prediction_dir)\n",
    "\n",
    "    all_auc = []\n",
    "    all_mcc = []\n",
    "    all_bal_acc = []\n",
    "    all_test_rels = []\n",
    "\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(os.path.join(prediction_dir, f))\n",
    "\n",
    "        if method_name == \"DeepLineDP\":\n",
    "            df = df[[\"train\", \"test\", \"filename\", \"target\", \"y_pred\", \"target\"]]\n",
    "            df = df.drop_duplicates()\n",
    "\n",
    "        file_level_result = get_file_level_metrics(df)\n",
    "\n",
    "        AUC = file_level_result[0]\n",
    "        MCC = file_level_result[1]\n",
    "        bal_acc = file_level_result[2]\n",
    "\n",
    "        all_auc.append(AUC)\n",
    "        all_mcc.append(MCC)\n",
    "        all_bal_acc.append(bal_acc)\n",
    "        all_test_rels.append(f.replace(\".csv\", \"\"))\n",
    "\n",
    "    result_df = pd.DataFrame({\"AUC\": all_auc, \"MCC\": all_mcc, \"Balance.Accuracy\": all_bal_acc})\n",
    "\n",
    "    all_test_rels = [rel.replace(\".csv\", \"\") for rel in all_test_rels]\n",
    "\n",
    "    result_df[\"release\"] = all_test_rels\n",
    "    result_df[\"technique\"] = method_name\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# bi_lstm_prediction_dir = \"../output/prediction/Bi-LSTM/\"\n",
    "# cnn_prediction_dir = \"../output/prediction/CNN/\"\n",
    "# dbn_prediction_dir = \"../output/prediction/DBN/\"\n",
    "# lr_prediction_dir = \"../output/prediction/LR/\"\n",
    "\n",
    "# bi_lstm_result = get_file_level_eval_result(bi_lstm_prediction_dir, \"Bi.LSTM\")\n",
    "# cnn_result = get_file_level_eval_result(cnn_prediction_dir, \"CNN\")\n",
    "# dbn_result = get_file_level_eval_result(dbn_prediction_dir, \"DBN\")\n",
    "# lr_result = get_file_level_eval_result(lr_prediction_dir, \"LR\")\n",
    "# deepline_dp_result = get_file_level_eval_result(prediction_dir, \"DeepLineDP\")\n",
    "\n",
    "# all_result = pd.concat([bi_lstm_result, cnn_result, dbn_result, lr_result, deepline_dp_result])\n",
    "\n",
    "# all_result.columns = [\"AUC\", \"MCC\", \"Balance.Accuracy\", \"Release\", \"Technique\"]\n",
    "\n",
    "# auc_result = all_result[[\"Technique\", \"AUC\"]]\n",
    "# auc_result = preprocess(auc_result, False)\n",
    "# auc_result.loc[auc_result[\"variable\"] == \"Bi.LSTM\", \"variable\"] = \"Bi-LSTM\"\n",
    "\n",
    "# mcc_result = all_result[[\"Technique\", \"MCC\"]]\n",
    "# mcc_result = preprocess(mcc_result, False)\n",
    "# mcc_result.loc[mcc_result[\"variable\"] == \"Bi.LSTM\", \"variable\"] = \"Bi-LSTM\"\n",
    "\n",
    "# bal_acc_result = all_result[[\"Technique\", \"Balance.Accuracy\"]]\n",
    "# bal_acc_result = preprocess(bal_acc_result, False)\n",
    "# bal_acc_result.loc[bal_acc_result[\"variable\"] == \"Bi.LSTM\", \"variable\"] = \"Bi-LSTM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ded20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Code for RQ3 -----------------------#\n",
    "\n",
    "def get_line_metrics_result(baseline_df, cur_df_file):\n",
    "    baseline_df_with_ground_truth = pd.merge(baseline_df, cur_df_file, on=[\"filename\", \"line.number\"])\n",
    "\n",
    "    sorted_df = baseline_df_with_ground_truth.groupby(\"filename\").apply(lambda x: x.sort_values(\"line.score\", ascending=False)).reset_index(drop=True)\n",
    "    sorted_df[\"order\"] = sorted_df.groupby(\"filename\").cumcount() + 1\n",
    "\n",
    "    # IFA\n",
    "    IFA = sorted_df[sorted_df[\"line.level.ground.truth\"] == \"True\"].groupby(\"filename\").apply(lambda x: x.nsmallest(1, \"order\")).reset_index(drop=True)\n",
    "\n",
    "    total_true = sorted_df.groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "\n",
    "    # Recall20%LOC\n",
    "    recall20LOC = sorted_df.groupby(\"filename\").apply(lambda x: x[x[\"order\"] <= int(0.2 * len(x))]).groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "    recall20LOC = recall20LOC.merge(total_true, on=\"filename\")\n",
    "    recall20LOC[\"recall20LOC\"] = recall20LOC[\"line.level.ground.truth\"] / recall20LOC[\"line.level.ground.truth_y\"]\n",
    "\n",
    "    # Effort20%Recall\n",
    "    effort20Recall = sorted_df.merge(total_true, on=\"filename\").groupby(\"filename\").apply(lambda x: sum(x[\"line.level.ground.truth\"].cumsum() / x[\"line.level.ground.truth_y\"] <= 0.2) / len(x)).reset_index()\n",
    "\n",
    "    ifa_list = IFA[\"order\"].tolist()\n",
    "    recall_list = recall20LOC[\"recall20LOC\"].tolist()\n",
    "    effort_list = effort20Recall[0].tolist()\n",
    "\n",
    "    result_df = pd.DataFrame({\"ifa_list\": ifa_list, \"recall_list\": recall_list, \"effort_list\": effort_list})\n",
    "\n",
    "    return result_df\n",
    "\n",
    "all_eval_releases = ['activemq-5.2.0', 'activemq-5.3.0', 'activemq-5.8.0', 'camel-2.10.0', 'camel-2.11.0', 'derby-10.5.1.1', 'groovy-1_6_BETA_2', 'hbase-0.95.2', 'hive-0.12.0', 'jruby-1.5.0', 'jruby-1.7.0.preview1', 'lucene-3.0.0', 'lucene-3.1', 'wicket-1.5.3']\n",
    "\n",
    "error_prone_result_dir = '../output/ErrorProne_result/'\n",
    "ngram_result_dir = '../output/n_gram_result/'\n",
    "rf_result_dir = '../output/RF-line-level-result/'\n",
    "\n",
    "n_gram_result_df = pd.DataFrame()\n",
    "error_prone_result_df = pd.DataFrame()\n",
    "rf_result_df = pd.DataFrame()\n",
    "\n",
    "for rel in all_eval_releases:\n",
    "    error_prone_result = pd.read_csv(os.path.join(error_prone_result_dir, rel + '-line-lvl-result.txt'), quotechar=\"\")\n",
    "    error_prone_result[\"EP_prediction_result\"] = error_prone_result[\"EP_prediction_result\"].replace({\"False\": 0, \"True\": 1})\n",
    "\n",
    "    n_gram_result = pd.read_csv(os.path.join(ngram_result_dir, rel + '-line-lvl-result.txt'), quotechar=\"\")\n",
    "    rf_result = pd.read_csv(os.path.join(rf_result_dir, rel + '-line-lvl-result.csv'))\n",
    "\n",
    "    n_gram_result = n_gram_result[[\"filename\", \"line.number\", \"line.score\"]]\n",
    "    rf_result = rf_result[[\"filename\", \"line_number\", \"line.score.pred\"]]\n",
    "\n",
    "    cur_df_file = df_all[df_all[\"test\"] == rel]\n",
    "    cur_df_file = cur_df_file[[\"filename\", \"line.number\", \"line.level.ground.truth\"]]\n",
    "\n",
    "    n_gram_eval_result = get_line_metrics_result(n_gram_result, cur_df_file)\n",
    "    error_prone_eval_result = get_line_metrics_result(error_prone_result, cur_df_file)\n",
    "    rf_eval_result = get_line_metrics_result(rf_result, cur_df_file)\n",
    "\n",
    "    n_gram_result_df = pd.concat([n_gram_result_df, n_gram_eval_result])\n",
    "    error_prone_result_df = pd.concat([error_prone_result_df, error_prone_eval_result])\n",
    "    rf_result_df = pd.concat([rf_result_df, rf_eval_result])\n",
    "\n",
    "# Force attention score of comment line is 0\n",
    "df_all.loc[df_all[\"is.comment.line\"] == \"True\", \"token.attention.score\"] = 0\n",
    "\n",
    "tmp_top_k = get_top_k_tokens(df_all, 1500)\n",
    "\n",
    "merged_df_all = pd.merge(df_all, tmp_top_k, on=[\"project\", \"train\", \"test\", \"filename\", \"token\"], how=\"left\")\n",
    "merged_df_all.loc[merged_df_all[\"flag\"].isna(), \"token.attention.score\"] = 0\n",
    "\n",
    "sum_line_attn = merged_df_all[(merged_df_all[\"file.level.ground.truth\"] == \"True\") & (merged_df_all[\"prediction.label\"] == \"True\")]\n",
    "sum_line_attn = sum_line_attn.groupby([\"test\", \"filename\", \"is.comment.line\", \"file.level.ground.truth\", \"prediction.label\", \"line.number\", \"line.level.ground.truth\"]).agg({\"token.attention.score\": \"sum\", \"num_tokens\": \"count\"}).reset_index()\n",
    "\n",
    "sorted_df = sum_line_attn.groupby([\"test\", \"filename\"]).apply(lambda x: x.sort_values(\"token.attention.score\", ascending=False)).reset_index(drop=True)\n",
    "sorted_df[\"order\"] = sorted_df.groupby([\"test\", \"filename\"]).cumcount() + 1\n",
    "\n",
    "# get result from DeepLineDP\n",
    "# calculate IFA\n",
    "IFA = sorted_df[sorted_df[\"line.level.ground.truth\"] == \"True\"].groupby([\"test\", \"filename\"]).apply(lambda x: x.nsmallest(1, \"order\")).reset_index(drop=True)\n",
    "\n",
    "total_true = sorted_df.groupby([\"test\", \"filename\"]).agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "\n",
    "# calculate Recall20%LOC\n",
    "recall20LOC = sorted_df.groupby([\"test\", \"filename\"]).apply(lambda x: x[x[\"order\"] <= int(0.2 * len(x))]).groupby([\"test\", \"filename\"]).agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "recall20LOC = recall20LOC.merge(total_true, on=[\"test\", \"filename\"])\n",
    "recall20LOC[\"recall20LOC\"] = recall20LOC[\"line.level.ground.truth\"] / recall20LOC[\"line.level.ground.truth_y\"]\n",
    "\n",
    "# calculate Effort20%Recall\n",
    "effort20Recall = sorted_df.merge(total_true, on=[\"test\", \"filename\"]).groupby([\"test\", \"filename\"]).apply(lambda x: sum(x[\"line.level.ground.truth\"].cumsum() / x[\"line.level.ground.truth_y\"] <= 0.2) / len(x)).reset_index()\n",
    "\n",
    "# prepare data for plotting\n",
    "deeplinedp_ifa = IFA[\"order\"].tolist()\n",
    "deeplinedp_recall = recall20LOC[\"recall20LOC\"].tolist()\n",
    "deeplinedp_effort = effort20Recall[0].tolist()\n",
    "\n",
    "deepline_dp_line_result = pd.DataFrame({\"IFA\": deeplinedp_ifa, \"Recall20%LOC\": deeplinedp_recall, \"Effort@20%Recall\": deeplinedp_effort})\n",
    "\n",
    "rf_result_df.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "n_gram_result_df.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "error_prone_result_df.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "deepline_dp_line_result.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "\n",
    "rf_result_df[\"technique\"] = \"RF\"\n",
    "n_gram_result_df[\"technique\"] = \"N.gram\"\n",
    "error_prone_result_df[\"technique\"] = \"ErrorProne\"\n",
    "deepline_dp_line_result[\"technique\"] = \"DeepLineDP\"\n",
    "\n",
    "all_line_result = pd.concat([rf_result_df, n_gram_result_df, error_prone_result_df, deepline_dp_line_result])\n",
    "\n",
    "recall_result_df = all_line_result[[\"technique\", \"Recall20%LOC\"]]\n",
    "ifa_result_df = all_line_result[[\"technique\", \"IFA\"]]\n",
    "effort_result_df = all_line_result[[\"technique\", \"Effort@20%Recall\"]]\n",
    "\n",
    "recall_result_df = preprocess(recall_result_df, False)\n",
    "ifa_result_df = preprocess(ifa_result_df, True)\n",
    "effort_result_df = preprocess(effort_result_df, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Code for RQ4 -----------------------#\n",
    "\n",
    "# get within-project result\n",
    "deepline_dp_result[\"project\"] = ['activemq', 'activemq', 'activemq', 'camel', 'camel', 'derby', 'groovy', 'hbase', 'hive', 'jruby', 'jruby', 'lucene', 'lucene', 'wicket']\n",
    "\n",
    "file_level_by_project = deepline_dp_result.groupby(\"project\").agg({\"all.auc\": \"mean\", \"all.mcc\": \"mean\", \"all.bal.acc\": \"mean\"}).reset_index()\n",
    "file_level_by_project.columns = [\"project\", \"AUC\", \"MCC\", \"Balance Accurracy\"]\n",
    "\n",
    "# get cross-project result\n",
    "prediction_dir = '../output/prediction/DeepLineDP/cross-release/'\n",
    "\n",
    "projs = ['activemq', 'camel', 'derby', 'groovy', 'hbase', 'hive', 'jruby', 'lucene', 'wicket']\n",
    "\n",
    "def get_line_level_metrics(df_all):\n",
    "    sum_line_attn = df_all[(df_all[\"file.level.ground.truth\"] == \"True\") & (df_all[\"prediction.label\"] == \"True\")]\n",
    "    sum_line_attn = sum_line_attn.groupby(\"filename\").agg({\"token.attention.score\": \"sum\", \"num_tokens\": \"count\"}).reset_index()\n",
    "\n",
    "    sorted_df = sum_line_attn.groupby(\"filename\").apply(lambda x: x.sort_values(\"token.attention.score\", ascending=False)).reset_index(drop=True)\n",
    "    sorted_df[\"order\"] = sorted_df.groupby(\"filename\").cumcount() + 1\n",
    "\n",
    "    # calculate IFA\n",
    "    IFA = sorted_df[sorted_df[\"line.level.ground.truth\"] == \"True\"].groupby(\"filename\").apply(lambda x: x.nsmallest(1, \"order\")).reset_index(drop=True)\n",
    "\n",
    "    total_true = sorted_df.groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "\n",
    "    # calculate Recall20%LOC\n",
    "    recall20LOC = sorted_df.groupby(\"filename\").apply(lambda x: x[x[\"order\"] <= int(0.2 * len(x))]).groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "    recall20LOC = recall20LOC.merge(total_true, on=\"filename\")\n",
    "    recall20LOC[\"recall20LOC\"] = recall20LOC[\"line.level.ground.truth\"] / recall20LOC[\"line.level.ground.truth_y\"]\n",
    "\n",
    "    # calculate Effort20%Recall\n",
    "    effort20Recall = sorted_df.merge(total_true, on=\"filename\").groupby(\"filename\").apply(lambda x: sum(x[\"line.level.ground.truth\"].cumsum() / x[\"line.level.ground.truth_y\"] <= 0.2) / len(x)).reset_index()\n",
    "\n",
    "    all_ifa = IFA[\"order\"].tolist()\n",
    "    all_recall = recall20LOC[\"recall20LOC\"].tolist()\n",
    "    all_effort = effort20Recall[0].tolist()\n",
    "\n",
    "    result_df = pd.DataFrame({\"all.ifa\": all_ifa, \"all.recall\": all_recall, \"all.effort\": all_effort})\n",
    "\n",
    "    return result_df\n",
    "\n",
    "all_line_result = pd.DataFrame()\n",
    "all_file_result = pd.DataFrame()\n",
    "\n",
    "for p in projs:\n",
    "    actual_pred_dir = os.path.join(prediction_dir, p)\n",
    "\n",
    "    all_files = os.listdir(actual_pred_dir)\n",
    "\n",
    "    all_auc = []\n",
    "    all_mcc = []\n",
    "    all_bal_acc = []\n",
    "    all_src_projs = []\n",
    "    all_tar_projs = []\n",
    "\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(os.path.join(actual_pred_dir, f))\n",
    "\n",
    "        f = f.replace(\".csv\", \"\")\n",
    "        f_split = f.split(\"-\")\n",
    "        target = f_split[-2]\n",
    "\n",
    "        df_file = df[[\"train\", \"test\", \"filename\", \"file.level.ground.truth\", \"prediction.prob\", \"prediction.label\"]]\n",
    "        df_file = df_file.drop_duplicates()\n",
    "\n",
    "        file_level_result = get_file_level_metrics(df_file)\n",
    "\n",
    "        AUC = file_level_result[0]\n",
    "        MCC = file_level_result[1]\n",
    "        bal_acc = file_level_result[2]\n",
    "\n",
    "        all_auc.append(AUC)\n",
    "        all_mcc.append(MCC)\n",
    "        all_bal_acc.append(bal_acc)\n",
    "\n",
    "        all_src_projs.append(p)\n",
    "        all_tar_projs.append(target)\n",
    "\n",
    "        tmp_top_k = get_top_k_tokens(df, 1500)\n",
    "\n",
    "        merged_df_all = pd.merge(df, tmp_top_k, on=[\"project\", \"train\", \"test\", \"filename\", \"token\"], how=\"left\")\n",
    "        merged_df_all.loc[merged_df_all[\"flag\"].isna(), \"token.attention.score\"] = 0\n",
    "\n",
    "        line_level_result = get_line_level_metrics(merged_df_all)\n",
    "        line_level_result[\"src\"] = p\n",
    "        line_level_result[\"target\"] = target\n",
    "\n",
    "        all_line_result = pd.concat([all_line_result, line_level_result])\n",
    "\n",
    "    file_level_result = pd.DataFrame({\"all.auc\": all_auc, \"all.mcc\": all_mcc, \"all.bal.acc\": all_bal_acc})\n",
    "    file_level_result[\"src\"] = p\n",
    "    file_level_result[\"target\"] = all_tar_projs\n",
    "\n",
    "    all_file_result = pd.concat([all_file_result, file_level_result])\n",
    "\n",
    "final_file_level_result = all_file_result.groupby(\"target\").agg({\"all.auc\": \"mean\", \"all.bal.acc\": \"mean\", \"all.mcc\": \"mean\"}).reset_index()\n",
    "final_line_level_result = all_line_result.groupby(\"target\").agg({\"all.recall\": \"mean\", \"all.effort\": \"mean\", \"all.ifa\": \"mean\"}).reset_index()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
