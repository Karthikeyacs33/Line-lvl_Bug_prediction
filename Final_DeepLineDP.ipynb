{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657d01d7-cb2d-4dfc-ac71-5f5df423ae88",
   "metadata": {},
   "source": [
    "# DeepLineDP Model for python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1228b86-c91c-4c3c-b808-ceabf8bdcdb8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "014f4efa-b688-4b82-9939-840564bad5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os, re, time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "import more_itertools\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "\n",
    "from sklearn.utils import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cffd9-d7f4-4cc0-a34e-4544931202a0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c60f84cd-9f0d-4aad-a080-6f239c872848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_line_random\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train.parquet.gzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath_to_line_random\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test.parquet.gzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m line : \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1825\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   1815\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   1816\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1822\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1823\u001b[0m     )\n\u001b[1;32m-> 1825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1468\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1460\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1461\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1462\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   1463\u001b[0m         ]\n\u001b[0;32m   1464\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1465\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   1466\u001b[0m         )\n\u001b[1;32m-> 1468\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_to_line_random = '.'\n",
    "\n",
    "train_df = pd.read_parquet(f'{path_to_line_random}/train.parquet.gzip')\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "test_df = pd.read_parquet(f'{path_to_line_random}/test.parquet.gzip')\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df['target'] = train_df['lines'].apply(lambda line : 0 if len(line) == 0 else 1)\n",
    "test_df['target'] = test_df['lines'].apply(lambda line : 0 if len(line) == 0 else 1)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95340d06-045e-457e-acd6-004b5f2b9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = train_df[train_df['target'] == 1].sample(75, random_state=42)\n",
    "train_df_0 = train_df[train_df['target'] == 0].sample(75, random_state=42)\n",
    "\n",
    "# Combine the DataFrames\n",
    "train_df = pd.concat([train_df_1, train_df_0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e3d1b-df3d-406b-a360-5f8ae9b6894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:55: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:55: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\mvska\\AppData\\Local\\Temp\\ipykernel_13132\\3109217446.py:55: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  code_line = re.sub('\\b\\d+\\b','',code_line)\n"
     ]
    }
   ],
   "source": [
    "# data_root_dir = '../datasets/original/'\n",
    "# save_dir = \"../datasets/preprocessed_data/\"\n",
    "\n",
    "char_to_remove = ['+','-','*','/','=','++','--','\\\\','<str>','<char>','|','&','!']\n",
    "\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# file_lvl_dir = data_root_dir+'File-level/'\n",
    "# line_lvl_dir = data_root_dir+'Line-level/'\n",
    "\n",
    "\n",
    "def is_comment_line(code_line, comments_list):\n",
    "    '''\n",
    "        input\n",
    "            code_line (string): source code in a line\n",
    "            comments_list (list): a list that contains every comments\n",
    "        output\n",
    "            boolean value\n",
    "    '''\n",
    "\n",
    "    code_line = code_line.strip()\n",
    "\n",
    "    if len(code_line) == 0:\n",
    "        return False\n",
    "    elif code_line.startswith('#'):\n",
    "        return True\n",
    "    elif code_line in comments_list:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def is_empty_line(code_line):\n",
    "    '''\n",
    "        input\n",
    "            code_line (string)\n",
    "        output\n",
    "            boolean value\n",
    "    '''\n",
    "\n",
    "    if len(code_line.strip()) == 0:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def preprocess_code_line(code_line):\n",
    "    '''\n",
    "        input\n",
    "            code_line (string)\n",
    "    '''\n",
    "\n",
    "    code_line = re.sub(\"\\'\\'\", \"\\'\", code_line)\n",
    "    code_line = re.sub(\"\\\".*?\\\"\", \"<str>\", code_line)\n",
    "    code_line = re.sub(\"\\'.*?\\'\", \"<char>\", code_line)\n",
    "    code_line = re.sub('\\b\\d+\\b','',code_line)\n",
    "    code_line = re.sub(\"\\\\[.*?\\\\]\", '', code_line)\n",
    "    code_line = re.sub(\"[\\\\.|,|:|;|{|}|(|)]\", ' ', code_line)\n",
    "\n",
    "    for char in char_to_remove:\n",
    "        code_line = code_line.replace(char,' ')\n",
    "\n",
    "    code_line = code_line.strip()\n",
    "\n",
    "    return code_line\n",
    "\n",
    "def preprocess_code(code_str):\n",
    "    '''\n",
    "        input\n",
    "            code_str (multi line str)\n",
    "    '''\n",
    "    if(code_str is None):\n",
    "        return ''\n",
    "    code_str = code_str.decode(\"latin-1\")\n",
    "    code_lines = code_str.splitlines()\n",
    "\n",
    "    preprocess_code_lines = []\n",
    "    is_comments = []\n",
    "    is_blank_line = []\n",
    "\n",
    "    # multi-line comments\n",
    "    comments = re.findall(r'(\"\"\"(.*?)\"\"\")|(\\'\\'\\'(.*?)\\'\\'\\')', code_str, re.DOTALL)\n",
    "    comments_temp = []\n",
    "    for tup in comments:\n",
    "        temp = ''\n",
    "        for s in tup:\n",
    "            temp += s\n",
    "        comments_temp.append(temp)\n",
    "    comments_str = '\\n'.join(comments_temp)\n",
    "    comments_list = comments_str.split('\\n')\n",
    "\n",
    "    for l in code_lines:\n",
    "        l = l.strip()\n",
    "        is_comment = is_comment_line(l,comments_list)\n",
    "        is_comments.append(is_comment)\n",
    "\n",
    "        if not is_comment:\n",
    "            l = preprocess_code_line(l)\n",
    "\n",
    "        preprocess_code_lines.append(l)\n",
    "\n",
    "    return ' \\n '.join(preprocess_code_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045a182-c766-4b08-b8d6-953d25a90490",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content'] = train_df['content'].apply(preprocess_code)\n",
    "train_df.to_csv('./preprocessed_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d499e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>content</th>\n",
       "      <th>methods</th>\n",
       "      <th>lines</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-23 23:09:24-01:00</td>\n",
       "      <td>a85808e3257c8b5ae906ecc7e816ffea19ce52b2</td>\n",
       "      <td>core</td>\n",
       "      <td>homeassistant\\components\\derivative\\sensor.py</td>\n",
       "      <td>\\n from decimal import Decimal  DecimalExcept...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14, 15, 16, 17, 49, 50, 51, 52, 53, 54, 67]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-02 02:03:19-01:00</td>\n",
       "      <td>42e2b801fe4cdb9e6fcff3c53b7d732bde59282b</td>\n",
       "      <td>airflow</td>\n",
       "      <td>airflow\\www\\views.py</td>\n",
       "      <td># \\n # Licensed to the Apache Software Foundat...</td>\n",
       "      <td>[_mark_task_instance_state, confirm]</td>\n",
       "      <td>[2224, 2286, 3599]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31 20:48:32-01:00</td>\n",
       "      <td>6cf57da89f199c5749974e141b4dba536bd57ee3</td>\n",
       "      <td>core</td>\n",
       "      <td>homeassistant\\components\\blink\\__init__.py</td>\n",
       "      <td>\\n import asyncio \\n from copy import deepcop...</td>\n",
       "      <td>[_reauth_flow_wrapper]</td>\n",
       "      <td>[19, 53]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-07 02:48:45-01:00</td>\n",
       "      <td>f7e1040236e088f4a0b5c725461cdf0eed80b068</td>\n",
       "      <td>lightning</td>\n",
       "      <td>pytorch_lightning\\trainer\\distrib_parts.py</td>\n",
       "      <td>\"\"\" \\n Lightning makes multi-gpu training and ...</td>\n",
       "      <td>[parse_gpu_ids]</td>\n",
       "      <td>[167, 168, 169, 170, 171, 172, 173, 174, 175, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-07 10:51:16-01:00</td>\n",
       "      <td>c1e51ef486cec17b69727b47452a34a7796d5677</td>\n",
       "      <td>ansible</td>\n",
       "      <td>lib\\ansible\\modules\\source_control\\github_webh...</td>\n",
       "      <td>#!/usr/bin/python \\n # \\n # Copyright: (c) 201...</td>\n",
       "      <td>[main]</td>\n",
       "      <td>[89, 94, 97, 131, 132]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                    commit  \\\n",
       "0 2020-02-23 23:09:24-01:00  a85808e3257c8b5ae906ecc7e816ffea19ce52b2   \n",
       "1 2022-03-02 02:03:19-01:00  42e2b801fe4cdb9e6fcff3c53b7d732bde59282b   \n",
       "2 2021-03-31 20:48:32-01:00  6cf57da89f199c5749974e141b4dba536bd57ee3   \n",
       "3 2019-12-07 02:48:45-01:00  f7e1040236e088f4a0b5c725461cdf0eed80b068   \n",
       "4 2019-02-07 10:51:16-01:00  c1e51ef486cec17b69727b47452a34a7796d5677   \n",
       "\n",
       "        repo                                           filepath  \\\n",
       "0       core      homeassistant\\components\\derivative\\sensor.py   \n",
       "1    airflow                               airflow\\www\\views.py   \n",
       "2       core         homeassistant\\components\\blink\\__init__.py   \n",
       "3  lightning         pytorch_lightning\\trainer\\distrib_parts.py   \n",
       "4    ansible  lib\\ansible\\modules\\source_control\\github_webh...   \n",
       "\n",
       "                                             content  \\\n",
       "0   \\n from decimal import Decimal  DecimalExcept...   \n",
       "1  # \\n # Licensed to the Apache Software Foundat...   \n",
       "2   \\n import asyncio \\n from copy import deepcop...   \n",
       "3  \"\"\" \\n Lightning makes multi-gpu training and ...   \n",
       "4  #!/usr/bin/python \\n # \\n # Copyright: (c) 201...   \n",
       "\n",
       "                                methods  \\\n",
       "0                                    []   \n",
       "1  [_mark_task_instance_state, confirm]   \n",
       "2                [_reauth_flow_wrapper]   \n",
       "3                       [parse_gpu_ids]   \n",
       "4                                [main]   \n",
       "\n",
       "                                               lines  target  \n",
       "0       [14, 15, 16, 17, 49, 50, 51, 52, 53, 54, 67]       1  \n",
       "1                                 [2224, 2286, 3599]       1  \n",
       "2                                           [19, 53]       1  \n",
       "3  [167, 168, 169, 170, 171, 172, 173, 174, 175, ...       1  \n",
       "4                             [89, 94, 97, 131, 132]       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5056a-54a0-4767-8e9c-78f06cd14c60",
   "metadata": {},
   "source": [
    "## Token Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca7b35-3b5a-4d88-8416-2f382e7dff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50\n",
    "def prepare_code2d(code_list):\n",
    "    '''\n",
    "        input\n",
    "            code_list (list): list that contains code each line (in str format)\n",
    "        output\n",
    "            code2d (nested list): a list that contains list of tokens with padding by '<pad>'\n",
    "    '''\n",
    "    # content to list(content)\n",
    "    code_list = str(code_list)\n",
    "    code_list = code_list.splitlines()\n",
    "    code2d = []\n",
    "\n",
    "    for c in code_list:\n",
    "        c = re.sub('\\\\s+',' ',c)\n",
    "\n",
    "        c = c.lower()\n",
    "\n",
    "        token_list = c.strip().split()\n",
    "        total_tokens = len(token_list)\n",
    "\n",
    "        token_list = token_list[:max_seq_len]\n",
    "\n",
    "        if total_tokens < max_seq_len:\n",
    "            token_list = token_list + ['<pad>']*(max_seq_len-total_tokens)\n",
    "\n",
    "        code2d.append(token_list)\n",
    "\n",
    "    return code2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae0f43-3066-4372-ad99-4d808aade4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content'] = train_df['content'].apply(prepare_code2d)\n",
    "train_df.to_csv('./token2d_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec76f0-9700-4e85-a494-3fb530a76df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.head(100)\n",
    "test_df['content'] = test_df['content'].apply(preprocess_code)\n",
    "test_df['content'] = test_df['content'].apply(prepare_code2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73527d3-f3f5-4c17-b381-d6eeb03b5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code3d_and_label(df):\n",
    "    '''\n",
    "        input\n",
    "            df (DataFrame): a dataframe from get_df()\n",
    "        output\n",
    "            code3d (nested list): a list of code2d from prepare_code2d()\n",
    "            all_file_label (list): a list of file-level label\n",
    "    '''\n",
    "    code_3d = prepare_code2d(train_df['content'].to_numpy())\n",
    "    all_file_label = train_df['target'].to_numpy().tolist()\n",
    "    return code_3d, all_file_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17b965-36f3-4501-a3dd-1f08182be238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(embedding_dim = 50):\n",
    "\n",
    "    w2v_path = './word2vec'\n",
    "\n",
    "    save_path = w2v_path+'/'+'w2v'+str(embedding_dim)+'dim.bin'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print('word2vec model at {} is already exists'.format(save_path))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(w2v_path):\n",
    "        os.makedirs(w2v_path)\n",
    "\n",
    "    # train_df = pd.read_csv('./token2d_train_df.csv') #uncomment to load saved file\n",
    "\n",
    "    train_code_3d, _ = get_code3d_and_label(train_df)\n",
    "\n",
    "    all_texts = list(more_itertools.collapse(train_code_3d[:],levels=2))\n",
    "\n",
    "    word2vec = Word2Vec(all_texts,vector_size=embedding_dim, min_count=1,sorted_vocab=1)\n",
    "\n",
    "    word2vec.save(save_path)\n",
    "    print('save word2vec model at path {} done'.format(save_path))\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e054fd5-a476-47d2-9ea5-ffc46ef0fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model at ./word2vec/w2v50dim.bin is already exists\n"
     ]
    }
   ],
   "source": [
    "word2vec = train_word2vec_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada5dd2-2ae6-436f-b8e3-cf78a1a1a3c2",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf999b9-0076-4070-a61b-42732896ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_x_vec(code_3d, word2vec):\n",
    "    x_vec = [[[\n",
    "        word2vec.wv.key_to_index[token] if token in word2vec.wv.key_to_index else len(word2vec.wv.key_to_index)\n",
    "        for token in text\n",
    "    ] for text in texts] for texts in code_3d]\n",
    "    return x_vec\n",
    "\n",
    "\n",
    "def pad_code(code_list_3d, max_sent_len, max_seq_len, limit_sent_len=True, mode='train'):\n",
    "  padded = []\n",
    "\n",
    "  for file in code_list_3d:\n",
    "    sent_list = []\n",
    "    for line in file:\n",
    "      new_line = line\n",
    "      # Truncate if line is longer than max_seq_len\n",
    "      if len(line) > max_seq_len:\n",
    "        new_line = line[:max_seq_len]\n",
    "          #edited here ..just trying\n",
    "      elif len(line) <= max_seq_len:\n",
    "          new_line = line + [0] * (max_seq_len - len(new_line))\n",
    "      sent_list.append(new_line)\n",
    "\n",
    "    # Pad the entire file (all sentences) to max_sent_len with zeros\n",
    "    padded_file = sent_list + [[0] * max_seq_len for _ in range(max_sent_len - len(sent_list))]\n",
    "\n",
    "    # If in training mode and `limit_sent_len` is True, keep only the first max_sent_len sentences\n",
    "    if mode == 'train' and limit_sent_len:\n",
    "        padded_file = padded_file[:max_sent_len]\n",
    "      \n",
    "    padded.append(padded_file)\n",
    "    # print(padded_file)\n",
    "  return padded\n",
    "\n",
    "\n",
    "def get_w2v_weight_for_deep_learning_models(word2vec_model, embed_dim):\n",
    "    word2vec_weights = torch.FloatTensor(word2vec_model.wv.vectors)\n",
    "    # add zero vector for unknown tokens\n",
    "    word2vec_weights = torch.cat((word2vec_weights, torch.zeros(1,embed_dim)))\n",
    "    return word2vec_weights\n",
    "    \n",
    "def get_dataloader(code_vec, label_list, batch_size, max_sent_len):\n",
    "  y_tensor = torch.FloatTensor([label for label in label_list])\n",
    "  \n",
    "  # Ensure padding happens to max_sent_len\n",
    "  code_vec_pad = pad_code(code_vec, max_sent_len,max_seq_len)\n",
    "  \n",
    "  # Print shapes for debugging (optional)\n",
    "  print(f\"code_vec shape: {len(code_vec)}\")\n",
    "  print(f\"Y_tensor shape: {len(y_tensor)}\")\n",
    "  print(f\"code_vec_pad shape: {len(code_vec_pad)}\")\n",
    "  \n",
    "  tensor_dataset = TensorDataset(torch.tensor(code_vec_pad), y_tensor)\n",
    "  dl = DataLoader(tensor_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "  return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f2f42-1548-40b7-8d55-70a5020cff7d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f848b-c7c8-4d7b-bd9f-ee2fdedfac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model structure\n",
    "class HierarchicalAttentionNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim, word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
    "        \"\"\"\n",
    "        vocab_size: number of words in the vocabulary of the model\n",
    "        embed_dim: dimension of word embeddings\n",
    "        word_gru_hidden_dim: dimension of word-level GRU; biGRU output is double this size\n",
    "        sent_gru_hidden_dim: dimension of sentence-level GRU; biGRU output is double this size\n",
    "        word_gru_num_layers: number of layers in word-level GRU\n",
    "        sent_gru_num_layers: number of layers in sentence-level GRU\n",
    "        word_att_dim: dimension of word-level attention layer\n",
    "        sent_att_dim: dimension of sentence-level attention layer\n",
    "        use_layer_norm: whether to use layer normalization\n",
    "        dropout: dropout rate; 0 to not use dropout\n",
    "        \"\"\"\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.sent_attention = SentenceAttention(\n",
    "            vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
    "            word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2 * sent_gru_hidden_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.use_layer_nome = use_layer_norm\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, code_tensor):\n",
    "        \n",
    "        code_lengths = []\n",
    "        sent_lengths = []\n",
    "\n",
    "        for file in code_tensor:\n",
    "            code_line = []\n",
    "            code_lengths.append(len(file))\n",
    "            for line in file:\n",
    "                code_line.append(len(line))\n",
    "            sent_lengths.append(code_line)\n",
    "        \n",
    "        code_tensor = code_tensor.type(torch.LongTensor)\n",
    "        code_lengths = torch.tensor(code_lengths).type(torch.LongTensor)\n",
    "        sent_lengths = torch.tensor(sent_lengths).type(torch.LongTensor)\n",
    "        \n",
    "        code_embeds, word_att_weights, sent_att_weights, sents = self.sent_attention(code_tensor, code_lengths, sent_lengths)\n",
    "\n",
    "        scores = self.fc(code_embeds)\n",
    "        final_scrs = self.sig(scores)\n",
    "\n",
    "        return final_scrs, word_att_weights, sent_att_weights, sents\n",
    "\n",
    "class SentenceAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentence-level attention module. Contains a word-level attention module.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
    "                word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
    "        super(SentenceAttention, self).__init__()\n",
    "\n",
    "        # Word-level attention module\n",
    "        self.word_attention = WordAttention(vocab_size, embed_dim, word_gru_hidden_dim, word_gru_num_layers, word_att_dim, use_layer_norm, dropout)\n",
    "\n",
    "        # Bidirectional sentence-level GRU\n",
    "        self.gru = nn.GRU(2 * word_gru_hidden_dim, sent_gru_hidden_dim, num_layers=sent_gru_num_layers,\n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(2 * sent_gru_hidden_dim, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Sentence-level attention\n",
    "        self.sent_attention = nn.Linear(2 * sent_gru_hidden_dim, sent_att_dim)\n",
    "\n",
    "        # Sentence context vector u_s to take dot product with\n",
    "        # This is equivalent to taking that dot product (Eq.10 in the paper),\n",
    "        # as u_s is the linear layer's 1D parameter vector here\n",
    "        self.sentence_context_vector = nn.Linear(sent_att_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, code_tensor, code_lengths, sent_lengths):\n",
    "\n",
    "        # Sort code_tensor by decreasing order in length\n",
    "        code_lengths, code_perm_idx = code_lengths.sort(dim=0, descending=True)\n",
    "        code_tensor = code_tensor[code_perm_idx]\n",
    "        sent_lengths = sent_lengths[code_perm_idx]\n",
    "\n",
    "        # Make a long batch of sentences by removing pad-sentences\n",
    "        # i.e. `code_tensor` was of size (num_code_tensor, padded_code_lengths, padded_sent_length)\n",
    "        # -> `packed_sents.data` is now of size (num_sents, padded_sent_length)\n",
    "        packed_sents = pack_padded_sequence(code_tensor, lengths=code_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # effective batch size at each timestep\n",
    "        valid_bsz = packed_sents.batch_sizes\n",
    "\n",
    "        # Make a long batch of sentence lengths by removing pad-sentences\n",
    "        # i.e. `sent_lengths` was of size (num_code_tensor, padded_code_lengths)\n",
    "        # -> `packed_sent_lengths.data` is now of size (num_sents)\n",
    "        packed_sent_lengths = pack_padded_sequence(sent_lengths, lengths=code_lengths.tolist(), batch_first=True)\n",
    "\n",
    "    \n",
    "    \n",
    "        # Word attention module\n",
    "        sents, word_att_weights = self.word_attention(packed_sents.data, packed_sent_lengths.data)\n",
    "\n",
    "        sents = self.dropout(sents)\n",
    "\n",
    "        # Sentence-level GRU over sentence embeddings\n",
    "        packed_sents, _ = self.gru(PackedSequence(sents, valid_bsz))\n",
    "\n",
    "        if self.use_layer_norm:\n",
    "            normed_sents = self.layer_norm(packed_sents.data)\n",
    "        else:\n",
    "            normed_sents = packed_sents\n",
    "\n",
    "        # Sentence attention\n",
    "        att = torch.tanh(self.sent_attention(normed_sents))\n",
    "        att = self.sentence_context_vector(att).squeeze(1)\n",
    "\n",
    "        val = att.max()\n",
    "        att = torch.exp(att - val)\n",
    "\n",
    "        # Restore as documents by repadding\n",
    "        att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
    "\n",
    "        sent_att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
    "\n",
    "        # Restore as documents by repadding\n",
    "        code_tensor, _ = pad_packed_sequence(packed_sents, batch_first=True)\n",
    "\n",
    "        # Compute document vectors\n",
    "        code_tensor = code_tensor * sent_att_weights.unsqueeze(2)\n",
    "        code_tensor = code_tensor.sum(dim=1)\n",
    "\n",
    "        # Restore as documents by repadding\n",
    "        word_att_weights, _ = pad_packed_sequence(PackedSequence(word_att_weights, valid_bsz), batch_first=True)\n",
    "\n",
    "        # Restore the original order of documents (undo the first sorting)\n",
    "        _, code_tensor_unperm_idx = code_perm_idx.sort(dim=0, descending=False)\n",
    "        code_tensor = code_tensor[code_tensor_unperm_idx]\n",
    "\n",
    "        word_att_weights = word_att_weights[code_tensor_unperm_idx]\n",
    "        sent_att_weights = sent_att_weights[code_tensor_unperm_idx]\n",
    "\n",
    "        return code_tensor, word_att_weights, sent_att_weights, sents\n",
    "\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Word-level attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, gru_hidden_dim, gru_num_layers, att_dim, use_layer_norm, dropout):\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # output (batch, hidden_size)\n",
    "        self.gru = nn.GRU(embed_dim, gru_hidden_dim, num_layers=gru_num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(2 * gru_hidden_dim, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Maps gru output to `att_dim` sized tensor\n",
    "        self.attention = nn.Linear(2 * gru_hidden_dim, att_dim)\n",
    "\n",
    "        # Word context vector (u_w) to take dot-product with\n",
    "        self.context_vector = nn.Linear(att_dim, 1, bias=False)\n",
    "\n",
    "    def init_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Initialized embedding layer with pretrained embeddings.\n",
    "        embeddings: embeddings to init with\n",
    "        \"\"\"\n",
    "        self.embeddings.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def freeze_embeddings(self, freeze=False):\n",
    "        \"\"\"\n",
    "        Set whether to freeze pretrained embeddings.\n",
    "        \"\"\"\n",
    "        self.embeddings.weight.requires_grad = freeze\n",
    "\n",
    "    def forward(self, sents, sent_lengths):\n",
    "        \"\"\"\n",
    "        sents: encoded sentence-level data; LongTensor (num_sents, pad_len, embed_dim)\n",
    "        return: sentence embeddings, attention weights of words\n",
    "        \"\"\"\n",
    "        # Sort sents by decreasing order in sentence lengths\n",
    "        sent_lengths, sent_perm_idx = sent_lengths.sort(dim=0, descending=True)\n",
    "        sents = sents[sent_perm_idx]\n",
    "\n",
    "        sents = self.embeddings(sents)\n",
    "\n",
    "        packed_words = pack_padded_sequence(sents, lengths=sent_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # effective batch size at each timestep\n",
    "        valid_bsz = packed_words.batch_sizes\n",
    "\n",
    "        # Apply word-level GRU over word embeddings\n",
    "        packed_words, _ = self.gru(packed_words)\n",
    "\n",
    "        if self.use_layer_norm:\n",
    "            normed_words = self.layer_norm(packed_words.data)\n",
    "        else:\n",
    "            normed_words = packed_words\n",
    "\n",
    "        # Word Attenton\n",
    "        att = torch.tanh(self.attention(normed_words.data))\n",
    "        att = self.context_vector(att).squeeze(1)\n",
    "\n",
    "        val = att.max()\n",
    "        att = torch.exp(att - val) # att.size: (n_words)\n",
    "\n",
    "        # Restore as sentences by repadding\n",
    "        att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
    "\n",
    "        att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
    "\n",
    "        # Restore as sentences by repadding\n",
    "        sents, _ = pad_packed_sequence(packed_words, batch_first=True)\n",
    "\n",
    "        # Compute sentence vectors\n",
    "        sents = sents * att_weights.unsqueeze(2)\n",
    "        sents = sents.sum(dim=1)\n",
    "\n",
    "        # Restore the original order of sentences (undo the first sorting)\n",
    "        _, sent_unperm_idx = sent_perm_idx.sort(dim=0, descending=False)\n",
    "        sents = sents[sent_unperm_idx]\n",
    "\n",
    "        att_weights = att_weights[sent_unperm_idx]\n",
    "\n",
    "        return sents, att_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212cf17-7c43-4ee4-906b-26ba1b694220",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cc30d-da19-4aa6-832c-87d17037997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# model setting\n",
    "batch_size = 32 #args.batch_size\n",
    "num_epochs = 10 #args.num_epochs\n",
    "max_grad_norm = 5\n",
    "embed_dim = 50 #args.embed_dim\n",
    "word_gru_hidden_dim = 64 #args.word_gru_hidden_dim\n",
    "sent_gru_hidden_dim = 64 #args.sent_gru_hidden_dim\n",
    "word_gru_num_layers = 2 #args.word_gru_num_layers\n",
    "sent_gru_num_layers = 2 #args.sent_gru_num_layers\n",
    "word_att_dim = 64\n",
    "sent_att_dim = 64\n",
    "use_layer_norm = True\n",
    "dropout = 0.1 #args.dropout\n",
    "lr = 0.001 #args.lr\n",
    "\n",
    "save_every_epochs = 1\n",
    "exp_name = ''#args.exp_name\n",
    "\n",
    "max_train_LOC = 900\n",
    "\n",
    "weight_dict = {}\n",
    "\n",
    "def get_loss_weight(labels):\n",
    "    '''\n",
    "        input\n",
    "            labels: a PyTorch tensor that contains labels\n",
    "        output\n",
    "            weight_tensor: a PyTorch tensor that contains weight of defect/clean class\n",
    "    '''\n",
    "    label_list = labels.cpu().numpy().squeeze().tolist()\n",
    "    weight_list = []\n",
    "\n",
    "    for lab in label_list:\n",
    "        if lab == 0:\n",
    "            weight_list.append(weight_dict['clean'])\n",
    "        else:\n",
    "            weight_list.append(weight_dict['defect'])\n",
    "\n",
    "    weight_tensor = torch.tensor(weight_list).reshape(-1,1)\n",
    "    return weight_tensor\n",
    "\n",
    "\n",
    "def train_model():\n",
    "\n",
    "\n",
    "    train_code3d, train_label = get_code3d_and_label(train_df)\n",
    "\n",
    "    sample_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_label), y = train_label)\n",
    "\n",
    "    weight_dict['defect'] = np.max(sample_weights)\n",
    "    weight_dict['clean'] = np.min(sample_weights)\n",
    "\n",
    "    word2vec = Word2Vec.load('./word2vec/w2v50dim.bin')\n",
    "    print('load Word2Vec finished')\n",
    "\n",
    "    word2vec_weights = get_w2v_weight_for_deep_learning_models(word2vec, embed_dim)\n",
    "\n",
    "    vocab_size = len(word2vec.wv.key_to_index) + 1  # Use key_to_index\n",
    "    # for unknown tokens\n",
    "\n",
    "    x_train_vec = get_x_vec(train_code3d, word2vec)\n",
    "    # x_valid_vec = get_x_vec(valid_code3d, word2vec)\n",
    "\n",
    "    max_sent_len = min(max([len(sent) for sent in (x_train_vec)]), max_train_LOC)\n",
    "\n",
    "    # print(x_train_vec[0])\n",
    "\n",
    "    train_dl = get_dataloader(x_train_vec,train_label,batch_size,max_sent_len)\n",
    "\n",
    "    # valid_dl = get_dataloader(x_valid_vec, valid_label,batch_size,max_sent_len)\n",
    "\n",
    "    model = HierarchicalAttentionNetwork(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        word_gru_hidden_dim=word_gru_hidden_dim,\n",
    "        sent_gru_hidden_dim=sent_gru_hidden_dim,\n",
    "        word_gru_num_layers=word_gru_num_layers,\n",
    "        sent_gru_num_layers=sent_gru_num_layers,\n",
    "        word_att_dim=word_att_dim,\n",
    "        sent_att_dim=sent_att_dim,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        dropout=dropout)\n",
    "\n",
    "    # model = model.cuda()\n",
    "    model.sent_attention.word_attention.freeze_embeddings(False)\n",
    "    \n",
    "    # print some word attention weights\n",
    "    for inputs, labels in train_dl:\n",
    "        output, word_att_weights, sent_att_weights, sents = model(inputs)\n",
    "        print(word_att_weights)\n",
    "        break\n",
    "    \n",
    "\n",
    "    optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "\n",
    "            # # inputs_cuda, labels_cuda = inputs.cuda(), labels.cuda()\n",
    "            # output, _, __, ___ = model(inputs)\n",
    "            # i need outputs and attention weights from model\n",
    "            output,word_att_weights,sent_att_weights,sents = model(inputs)\n",
    "\n",
    "            weight_tensor = get_loss_weight(labels)\n",
    "\n",
    "            criterion.weight = weight_tensor\n",
    "\n",
    "            loss = criterion(output, labels.reshape(batch_size,1))\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch}: {time.time()-start}')\n",
    "\n",
    "    torch.save(model.state_dict(), './DeepLineDPReplication.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3d83b-f314-4b33-b60f-fe72e490281f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Word2Vec finished\n",
      "code_vec shape: 150\n",
      "Y_tensor shape: 150\n",
      "code_vec_pad shape: 150\n",
      "tensor([[[0.0136, 0.0174, 0.0138,  ..., 0.0237, 0.0219, 0.0249],\n",
      "         [0.0211, 0.0208, 0.0188,  ..., 0.0231, 0.0219, 0.0206],\n",
      "         [0.0203, 0.0185, 0.0163,  ..., 0.0223, 0.0238, 0.0254],\n",
      "         ...,\n",
      "         [0.0215, 0.0236, 0.0186,  ..., 0.0211, 0.0221, 0.0227],\n",
      "         [0.0229, 0.0207, 0.0185,  ..., 0.0203, 0.0212, 0.0213],\n",
      "         [0.0205, 0.0222, 0.0191,  ..., 0.0220, 0.0218, 0.0250]],\n",
      "\n",
      "        [[0.0166, 0.0195, 0.0155,  ..., 0.0233, 0.0234, 0.0262],\n",
      "         [0.0198, 0.0232, 0.0197,  ..., 0.0207, 0.0234, 0.0236],\n",
      "         [0.0201, 0.0200, 0.0176,  ..., 0.0209, 0.0230, 0.0216],\n",
      "         ...,\n",
      "         [0.0194, 0.0184, 0.0171,  ..., 0.0207, 0.0214, 0.0251],\n",
      "         [0.0202, 0.0196, 0.0166,  ..., 0.0237, 0.0272, 0.0241],\n",
      "         [0.0217, 0.0189, 0.0183,  ..., 0.0211, 0.0221, 0.0227]],\n",
      "\n",
      "        [[0.0147, 0.0188, 0.0166,  ..., 0.0221, 0.0251, 0.0278],\n",
      "         [0.0220, 0.0198, 0.0177,  ..., 0.0203, 0.0205, 0.0222],\n",
      "         [0.0206, 0.0199, 0.0169,  ..., 0.0237, 0.0227, 0.0240],\n",
      "         ...,\n",
      "         [0.0201, 0.0178, 0.0164,  ..., 0.0213, 0.0220, 0.0222],\n",
      "         [0.0229, 0.0207, 0.0174,  ..., 0.0213, 0.0227, 0.0230],\n",
      "         [0.0213, 0.0202, 0.0170,  ..., 0.0214, 0.0221, 0.0237]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0150, 0.0173, 0.0150,  ..., 0.0221, 0.0231, 0.0266],\n",
      "         [0.0213, 0.0190, 0.0180,  ..., 0.0227, 0.0227, 0.0236],\n",
      "         [0.0219, 0.0202, 0.0164,  ..., 0.0209, 0.0207, 0.0231],\n",
      "         ...,\n",
      "         [0.0215, 0.0212, 0.0184,  ..., 0.0218, 0.0226, 0.0237],\n",
      "         [0.0200, 0.0193, 0.0173,  ..., 0.0203, 0.0211, 0.0222],\n",
      "         [0.0208, 0.0201, 0.0189,  ..., 0.0201, 0.0215, 0.0231]],\n",
      "\n",
      "        [[0.0155, 0.0185, 0.0160,  ..., 0.0205, 0.0220, 0.0229],\n",
      "         [0.0231, 0.0230, 0.0199,  ..., 0.0224, 0.0226, 0.0237],\n",
      "         [0.0211, 0.0221, 0.0172,  ..., 0.0212, 0.0209, 0.0210],\n",
      "         ...,\n",
      "         [0.0209, 0.0199, 0.0180,  ..., 0.0205, 0.0234, 0.0239],\n",
      "         [0.0208, 0.0205, 0.0180,  ..., 0.0220, 0.0235, 0.0257],\n",
      "         [0.0206, 0.0222, 0.0200,  ..., 0.0208, 0.0213, 0.0235]],\n",
      "\n",
      "        [[0.0162, 0.0186, 0.0158,  ..., 0.0213, 0.0225, 0.0235],\n",
      "         [0.0171, 0.0167, 0.0156,  ..., 0.0215, 0.0231, 0.0259],\n",
      "         [0.0158, 0.0146, 0.0177,  ..., 0.0229, 0.0222, 0.0240],\n",
      "         ...,\n",
      "         [0.0189, 0.0197, 0.0172,  ..., 0.0211, 0.0241, 0.0238],\n",
      "         [0.0213, 0.0230, 0.0200,  ..., 0.0209, 0.0218, 0.0235],\n",
      "         [0.0211, 0.0203, 0.0177,  ..., 0.0203, 0.0204, 0.0243]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Epoch 0: 13.005314111709595\n",
      "Epoch 1: 11.906357526779175\n",
      "Epoch 2: 11.64940357208252\n",
      "Epoch 3: 12.249281167984009\n",
      "Epoch 4: 13.338040828704834\n",
      "Epoch 5: 12.379257917404175\n",
      "Epoch 6: 11.67007064819336\n",
      "Epoch 7: 11.1191565990448\n",
      "Epoch 8: 12.286519050598145\n",
      "Epoch 9: 11.439123153686523\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2210344-f737-40dd-8ea4-93a88b68b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(test_df)\n",
    "torch.manual_seed(0)\n",
    "embed_dim = 50 #args.embed_dim\n",
    "word_gru_hidden_dim = 64 #args.word_gru_hidden_dim\n",
    "sent_gru_hidden_dim = 64 #args.sent_gru_hidden_dim\n",
    "word_gru_num_layers = 2 #args.word_gru_num_layers\n",
    "sent_gru_num_layers = 2 #args.sent_gru_num_layers\n",
    "word_att_dim = 64\n",
    "sent_att_dim = 64\n",
    "use_layer_norm = True\n",
    "dropout = 0.1 #args.dropout\n",
    "lr = 0.001 #args.lr\n",
    "max_test_LOC = 900\n",
    "\n",
    "def test_model():\n",
    "\n",
    "\n",
    "    test_code3d, test_label = get_code3d_and_label(test_df)\n",
    "\n",
    "    sample_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(test_label), y = test_label)\n",
    "\n",
    "    weight_dict['defect'] = np.max(sample_weights)\n",
    "    weight_dict['clean'] = np.min(sample_weights)\n",
    "\n",
    "    word2vec = Word2Vec.load('./word2vec/w2v50dim.bin')\n",
    "    print('load Word2Vec finished')\n",
    "\n",
    "    word2vec_weights = get_w2v_weight_for_deep_learning_models(word2vec, embed_dim)\n",
    "\n",
    "    vocab_size = len(word2vec.wv.key_to_index) + 1  # Use key_to_index\n",
    "    # for unknown tokens\n",
    "\n",
    "    x_test_vec = get_x_vec(test_code3d, word2vec)\n",
    "\n",
    "    max_sent_len = min(max([len(sent) for sent in (x_test_vec)]), max_test_LOC)\n",
    "\n",
    "    test_dl = get_dataloader(x_test_vec,test_label,batch_size,max_sent_len)\n",
    "\n",
    "    loaded_dict = torch.load('./DeepLineDPReplication.pth')\n",
    "    model = HierarchicalAttentionNetwork(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        word_gru_hidden_dim=word_gru_hidden_dim,\n",
    "        sent_gru_hidden_dim=sent_gru_hidden_dim,\n",
    "        word_gru_num_layers=word_gru_num_layers,\n",
    "        sent_gru_num_layers=sent_gru_num_layers,\n",
    "        word_att_dim=word_att_dim,\n",
    "        sent_att_dim=sent_att_dim,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    model.load_state_dict(loaded_dict)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in test_dl:\n",
    "    #     outputs, _, __, ___ = model(inputs)\n",
    "        outputs, word_att_weights, sent_att_weights, sents = model(inputs)\n",
    "        return outputs,word_att_weights,sent_att_weights, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61992c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>content</th>\n",
       "      <th>methods</th>\n",
       "      <th>lines</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-13 15:35:10+04:00</td>\n",
       "      <td>000fbe63d390c59b9c1e29216c35fc52b991f2f3</td>\n",
       "      <td>lightning</td>\n",
       "      <td>pytorch_lightning\\trainer\\connectors\\logger_co...</td>\n",
       "      <td>[[#, copyright, the, pytorch, lightning, team....</td>\n",
       "      <td>[extract_batch_size, _extract_batch_size]</td>\n",
       "      <td>[17, 24, 593]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-11 13:12:55+04:00</td>\n",
       "      <td>038d5338530411bb47283fda1e84dec91137880b</td>\n",
       "      <td>localstack</td>\n",
       "      <td>localstack\\aws\\app.py</td>\n",
       "      <td>[[import, logging, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;,...</td>\n",
       "      <td>[__init__]</td>\n",
       "      <td>[62]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-16 08:51:12+04:00</td>\n",
       "      <td>0786e84a33155ebc8d8d3502e3a7f3060b86a4ec</td>\n",
       "      <td>scrapy</td>\n",
       "      <td>scrapy\\utils\\iterators.py</td>\n",
       "      <td>[[import, re, csv, six, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;...</td>\n",
       "      <td>[csviter]</td>\n",
       "      <td>[3, 4, 5, 6, 7, 55]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-28 17:22:24+04:00</td>\n",
       "      <td>0094cb0d0472b08f92915e948907b237eea020e3</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>spacy\\cli\\train.py</td>\n",
       "      <td>[[from, typing, import, optional, dict, any, t...</td>\n",
       "      <td>[update_meta]</td>\n",
       "      <td>[449]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-18 21:30:15+04:00</td>\n",
       "      <td>03c6f4bf250edd18eb818ed65090f508636b0bff</td>\n",
       "      <td>localstack</td>\n",
       "      <td>localstack\\services\\awslambda\\lambda_executors.py</td>\n",
       "      <td>[[import, os, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[30, 31, 33, 37, 39]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                    commit  \\\n",
       "0 2021-07-13 15:35:10+04:00  000fbe63d390c59b9c1e29216c35fc52b991f2f3   \n",
       "1 2022-07-11 13:12:55+04:00  038d5338530411bb47283fda1e84dec91137880b   \n",
       "2 2014-07-16 08:51:12+04:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
       "3 2020-07-28 17:22:24+04:00  0094cb0d0472b08f92915e948907b237eea020e3   \n",
       "4 2020-01-18 21:30:15+04:00  03c6f4bf250edd18eb818ed65090f508636b0bff   \n",
       "\n",
       "         repo                                           filepath  \\\n",
       "0   lightning  pytorch_lightning\\trainer\\connectors\\logger_co...   \n",
       "1  localstack                              localstack\\aws\\app.py   \n",
       "2      scrapy                          scrapy\\utils\\iterators.py   \n",
       "3       spaCy                                 spacy\\cli\\train.py   \n",
       "4  localstack  localstack\\services\\awslambda\\lambda_executors.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  [[#, copyright, the, pytorch, lightning, team....   \n",
       "1  [[import, logging, <pad>, <pad>, <pad>, <pad>,...   \n",
       "2  [[import, re, csv, six, <pad>, <pad>, <pad>, <...   \n",
       "3  [[from, typing, import, optional, dict, any, t...   \n",
       "4  [[import, os, <pad>, <pad>, <pad>, <pad>, <pad...   \n",
       "\n",
       "                                     methods                 lines  target  \n",
       "0  [extract_batch_size, _extract_batch_size]         [17, 24, 593]       1  \n",
       "1                                 [__init__]                  [62]       1  \n",
       "2                                  [csviter]   [3, 4, 5, 6, 7, 55]       1  \n",
       "3                              [update_meta]                 [449]       1  \n",
       "4                                         []  [30, 31, 33, 37, 39]       1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f465cd-8e70-451a-ac76-17c3809b03d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Word2Vec finished\n",
      "code_vec shape: 150\n",
      "Y_tensor shape: 150\n",
      "code_vec_pad shape: 150\n"
     ]
    }
   ],
   "source": [
    "y_pred,word_att_weights,sent_att_weights, y_gt = test_model() # test --> changed to train\n",
    "# y_pred, y_gt = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2644, 0.0440, 0.0272,  ..., 0.0113, 0.0107, 0.0104],\n",
      "        [0.5365, 0.0208, 0.0137,  ..., 0.0074, 0.0070, 0.0068],\n",
      "        [0.5585, 0.0212, 0.0136,  ..., 0.0070, 0.0066, 0.0064],\n",
      "        ...,\n",
      "        [0.5578, 0.0783, 0.0183,  ..., 0.0059, 0.0055, 0.0054],\n",
      "        [0.6976, 0.1461, 0.0348,  ..., 0.0015, 0.0014, 0.0014],\n",
      "        [0.5365, 0.0208, 0.0137,  ..., 0.0074, 0.0070, 0.0068]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sent_att_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d31947-6c9e-47df-9cb8-2cc47895bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.detach().numpy()\n",
    "word_att_weights = word_att_weights.detach().numpy()\n",
    "sent_att_weights = sent_att_weights.detach().numpy()\n",
    "y_gt = y_gt.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8d66c-16f0-4392-bacb-511abdf6e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = np.array([prob[0] for prob in y_pred])\n",
    "word_att_weights = np.array([att[0] for att in word_att_weights])\n",
    "sent_att_weights = np.array([att[0] for att in sent_att_weights])\n",
    "y_pred = np.where(y_probs >= 0.45, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffb3f6-d604-4d5f-b466-722fcf47fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e477e6a-062d-4114-8de3-3ad276db5993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.49\n",
      "Recall: 1.0\n",
      "F1-score: 0.6577181208053692\n",
      "False Alarm Rate: 1.0\n",
      "Distance to Heaven: 0.7071067811865476\n",
      "AUC: 0.49279711884753896\n"
     ]
    }
   ],
   "source": [
    "# precision, Recall, F1-score, Confusion matrix, False Alarm Rate, Distance-to-Heaven, AUC\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "\n",
    "prec, rec, f1, _ = metrics.precision_recall_fscore_support(y_gt,y_pred,average='binary') # at threshold = 0.5\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_gt, y_pred, labels=[0, 1]).ravel()\n",
    "FAR = fp/(fp+tn)\n",
    "dist_heaven = math.sqrt((pow(1-rec,2)+pow(0-FAR,2))/2.0)\n",
    "AUC = metrics.roc_auc_score(y_gt, y_probs)\n",
    "\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"False Alarm Rate: {FAR}\")\n",
    "print(f\"Distance to Heaven: {dist_heaven}\")\n",
    "print(f\"AUC: {AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a18e72-472b-4679-a3dc-71d2d79c2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./toscore_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6671cb5-61b4-4d1a-9166-8f03e1bc8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'commit', 'repo', 'filepath', 'content', 'methods', 'lines',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b4918-5c20-47a2-b0cf-5330e4580e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save.fig.dir\n",
    "save_fig_dir = '../output/figure/'\n",
    "os.makedirs(save_fig_dir, exist_ok=True)\n",
    "\n",
    "def preprocess(x, reverse):\n",
    "    x.columns = [\"variable\",\"value\"]\n",
    "    tmp = pd.concat([x[\"variable\"], x[\"value\"]], axis=1)\n",
    "    tmp = tmp.pivot(columns=\"variable\", values=\"value\")\n",
    "    tmp.columns = tmp.columns.str.replace(\".value\", \"\")\n",
    "    df = tmp\n",
    "    ranking = None\n",
    "    \n",
    "    if reverse == True:\n",
    "        ranking = (max(df.columns) - df.columns) + 1\n",
    "    else:\n",
    "        ranking = df.columns\n",
    "    \n",
    "    df[\"rank\"] = \"Rank\" + ranking.astype(str)\n",
    "    return df\n",
    "\n",
    "def get_top_k_tokens(df, k):\n",
    "    top_k = df[(df[\"is.comment.line\"] == \"False\") & (df[\"file.level.ground.truth\"] == \"True\") & (df[\"prediction.label\"] == \"True\")]\n",
    "    top_k = top_k.groupby([\"test\", \"filename\"]).apply(lambda x: x.nlargest(k, \"token.attention.score\")).reset_index(drop=True)\n",
    "    top_k = top_k[[\"project\", \"train\", \"test\", \"filename\", \"token\"]].drop_duplicates()\n",
    "    top_k[\"flag\"] = \"topk\"\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4feae2-0ea0-4ab2-ac8a-a3e35afa8cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>commit</th>\n",
       "      <th>repo</th>\n",
       "      <th>filepath</th>\n",
       "      <th>content</th>\n",
       "      <th>methods</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-13 15:35:10+04:00</td>\n",
       "      <td>000fbe63d390c59b9c1e29216c35fc52b991f2f3</td>\n",
       "      <td>lightning</td>\n",
       "      <td>pytorch_lightning\\trainer\\connectors\\logger_co...</td>\n",
       "      <td>b'# Copyright The PyTorch Lightning team.\\n#\\n...</td>\n",
       "      <td>[extract_batch_size, _extract_batch_size]</td>\n",
       "      <td>[17, 24, 593]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-11 13:12:55+04:00</td>\n",
       "      <td>038d5338530411bb47283fda1e84dec91137880b</td>\n",
       "      <td>localstack</td>\n",
       "      <td>localstack\\aws\\app.py</td>\n",
       "      <td>b'import logging\\n\\nfrom localstack.aws import...</td>\n",
       "      <td>[__init__]</td>\n",
       "      <td>[62]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-16 08:51:12+04:00</td>\n",
       "      <td>0786e84a33155ebc8d8d3502e3a7f3060b86a4ec</td>\n",
       "      <td>scrapy</td>\n",
       "      <td>scrapy\\utils\\iterators.py</td>\n",
       "      <td>b'import re, csv, six\\n\\ntry:\\n    from cStrin...</td>\n",
       "      <td>[csviter]</td>\n",
       "      <td>[3, 4, 5, 6, 7, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-28 17:22:24+04:00</td>\n",
       "      <td>0094cb0d0472b08f92915e948907b237eea020e3</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>spacy\\cli\\train.py</td>\n",
       "      <td>b'from typing import Optional, Dict, Any, Tupl...</td>\n",
       "      <td>[update_meta]</td>\n",
       "      <td>[449]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-18 21:30:15+04:00</td>\n",
       "      <td>03c6f4bf250edd18eb818ed65090f508636b0bff</td>\n",
       "      <td>localstack</td>\n",
       "      <td>localstack\\services\\awslambda\\lambda_executors.py</td>\n",
       "      <td>b'import os\\nimport re\\nimport glob\\nimport js...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[30, 31, 33, 37, 39]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                    commit  \\\n",
       "0 2021-07-13 15:35:10+04:00  000fbe63d390c59b9c1e29216c35fc52b991f2f3   \n",
       "1 2022-07-11 13:12:55+04:00  038d5338530411bb47283fda1e84dec91137880b   \n",
       "2 2014-07-16 08:51:12+04:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
       "3 2020-07-28 17:22:24+04:00  0094cb0d0472b08f92915e948907b237eea020e3   \n",
       "4 2020-01-18 21:30:15+04:00  03c6f4bf250edd18eb818ed65090f508636b0bff   \n",
       "\n",
       "         repo                                           filepath  \\\n",
       "0   lightning  pytorch_lightning\\trainer\\connectors\\logger_co...   \n",
       "1  localstack                              localstack\\aws\\app.py   \n",
       "2      scrapy                          scrapy\\utils\\iterators.py   \n",
       "3       spaCy                                 spacy\\cli\\train.py   \n",
       "4  localstack  localstack\\services\\awslambda\\lambda_executors.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  b'# Copyright The PyTorch Lightning team.\\n#\\n...   \n",
       "1  b'import logging\\n\\nfrom localstack.aws import...   \n",
       "2  b'import re, csv, six\\n\\ntry:\\n    from cStrin...   \n",
       "3  b'from typing import Optional, Dict, Any, Tupl...   \n",
       "4  b'import os\\nimport re\\nimport glob\\nimport js...   \n",
       "\n",
       "                                     methods                 lines  \n",
       "0  [extract_batch_size, _extract_batch_size]         [17, 24, 593]  \n",
       "1                                 [__init__]                  [62]  \n",
       "2                                  [csviter]   [3, 4, 5, 6, 7, 55]  \n",
       "3                              [update_meta]                 [449]  \n",
       "4                                         []  [30, 31, 33, 37, 39]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_deep_random = '.'\n",
    "test = pd.read_parquet(f'{path_to_deep_random}/test.parquet.gzip')\n",
    "test = test.reset_index(drop=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"./init_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050839b-811e-434e-985f-331997bdab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17  24 593]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x=test['content'][0]\n",
    "\n",
    "def process_byte_column(df, i):\n",
    "    text_data = df['content'].iloc[i].decode('latin-1', errors='replace')\n",
    "    lines = text_data.split('\\n')\n",
    "    return lines\n",
    "print(test['lines'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b41a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column which is a array label for comment lines\n",
    "is_comment = False\n",
    "columns = ['datetime', 'commit', 'repo', 'filepath', 'is_comment', 'line', 'is_buggy','attention_score','file_target','ypred_file']\n",
    "tdf = pd.DataFrame(columns=columns)\n",
    "# tdf.columns = ['datetime', 'commit', 'repo', 'filepath', 'is_comment', 'line', 'is_buggy']\n",
    "row_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c647bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'commit', 'repo', 'filepath', 'is_comment', 'line',\n",
      "       'is_buggy', 'attention_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(tdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ce461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0124259265\n"
     ]
    }
   ],
   "source": [
    "print((word_att_weights[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaa101-842d-4bc3-a722-37bf8992ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(train)):\n",
    "print(len(test))\n",
    "for i in range(len(test)):\n",
    "    t_temp =[]\n",
    "    \n",
    "    t_temp.append(test['target'][i])\n",
    "    temp = process_byte_column(test, i)\n",
    "    j = 0  # line number in the file\n",
    "    if i % 100:\n",
    "        print(f'doc num:{i}')\n",
    "    is_comment = False\n",
    "    for line in temp:\n",
    "        \n",
    "        j += 1\n",
    "        row_dict = []\n",
    "        row_dict.append(test['datetime'][i])\n",
    "        row_dict.append(test['commit'][i])\n",
    "        row_dict.append(test['repo'][i])\n",
    "        row_dict.append(test['filepath'][i])\n",
    "        if line.startswith('#'):\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif line.startswith(\"'''\"):\n",
    "            is_comment = True\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif is_comment:\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif line.endswith(\"'''\") and is_comment:\n",
    "            is_comment = False\n",
    "            row_dict.append(True)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False)\n",
    "        elif line == '':\n",
    "            continue\n",
    "        else:\n",
    "            row_dict.append(False)\n",
    "            row_dict.append(line)\n",
    "            row_dict.append(False if j not in test['lines'][i] else True)\n",
    "        # tdf[attention_score]. Each word score must be updated from word attention weights in the test model output\n",
    "        if j < len(word_att_weights):\n",
    "            row_dict.append([word_att_weights[j][k]\n",
    "                            for k in range(len(word_att_weights[j]))])\n",
    "        else:\n",
    "            row_dict.append(None)  # or some other default value\n",
    "        \n",
    "        # fill tdf[file_target] with the y_gt from test_df\n",
    "        row_dict.append(y_gt[i])\n",
    "        # fill tdf[ypred_file] with the y_pred from test_df\n",
    "        row_dict.append(y_pred[i])\n",
    "        tdf.loc[len(tdf.index)] = row_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887de08-50a1-49a2-a2b7-408815c13e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        True\n",
      "1        True\n",
      "2        True\n",
      "3        True\n",
      "4        True\n",
      "        ...  \n",
      "8550    False\n",
      "8551    False\n",
      "8552    False\n",
      "8553     True\n",
      "8554     True\n",
      "Name: is_comment, Length: 8555, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(tdf['is_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93507f9d-e00b-4a47-aa21-6535f99f00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdf.to_csv(\"./line_lvl_eval.csv\")\n",
    "# load line_lvl_eval.csv to tdf dataframe\n",
    "tdf.to_csv(\"./tdf_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce7875",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../output/prediction/DeepLineDP/within-release/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_k\n\u001b[0;32m     33\u001b[0m prediction_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/prediction/DeepLineDP/within-release/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 34\u001b[0m all_files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame() \n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../output/prediction/DeepLineDP/within-release/'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "# Set save.fig.dir\n",
    "save_fig_dir = './output/figure/'\n",
    "os.makedirs(save_fig_dir, exist_ok=True)\n",
    "\n",
    "def preprocess(x, reverse):\n",
    "    x.columns = [\"variable\",\"value\"]\n",
    "    tmp = pd.concat([x[\"variable\"], x[\"value\"]], axis=1)\n",
    "    tmp = tmp.pivot(columns=\"variable\", values=\"value\")\n",
    "    tmp.columns = tmp.columns.str.replace(\".value\", \"\")\n",
    "    df = tmp\n",
    "    ranking = None\n",
    "    \n",
    "    if reverse == True:\n",
    "        ranking = (max(df.columns) - df.columns) + 1\n",
    "    else:\n",
    "        ranking = df.columns\n",
    "    \n",
    "    df[\"rank\"] = \"Rank\" + ranking.astype(str)\n",
    "    return df\n",
    "\n",
    "def get_top_k_tokens(df, k):\n",
    "    top_k = df[(df[\"is_comment\"] == \"False\") & (df[\"target\"] == \"True\") & (df[\"is_buggy\"] == \"True\")]\n",
    "    top_k = top_k.groupby([\"test\", \"filename\"]).apply(lambda x: x.nlargest(k, \"token.attention.score\")).reset_index(drop=True)\n",
    "    top_k = top_k[[\"project\", \"train\", \"test\", \"filename\", \"token\"]].drop_duplicates()\n",
    "    top_k[\"flag\"] = \"topk\"\n",
    "    return top_k\n",
    "\n",
    "prediction_dir = '../output/prediction/DeepLineDP/within-release/'\n",
    "all_files = os.listdir(prediction_dir)\n",
    "\n",
    "df_all = pd.DataFrame() \n",
    "\n",
    "for f in all_files:\n",
    "    df = pd.read_csv(os.path.join(prediction_dir, f))\n",
    "    df_all = pd.concat([df_all, df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0ea4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is_comment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'is_comment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------- Code for RQ1 -----------------------#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m##df_all will be tdf dataframe\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# RQ1-1\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_to_plot \u001b[38;5;241m=\u001b[39m df_all[(\u001b[43mdf_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_comment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (df_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (df_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# df_to_plot = df_to_plot.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": [\"max\", \"min\", \"sd\"]}).reset_index()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_to_plot \u001b[38;5;241m=\u001b[39m df_to_plot\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m,])\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken.attention.score\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\mvska\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'is_comment'"
     ]
    }
   ],
   "source": [
    "# ---------------- Code for RQ1 -----------------------#\n",
    "\n",
    "##df_all will be tdf dataframe\n",
    "\n",
    "# RQ1-1\n",
    "df_to_plot = df_all[(df_all[\"is_comment\"] == \"False\") & (df_all[\"target\"] == \"True\") & (df_all[\"y_pred\"] == \"True\")]\n",
    "# df_to_plot = df_to_plot.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": [\"max\", \"min\", \"sd\"]}).reset_index()\n",
    "df_to_plot = df_to_plot.groupby([\"datetime\",\"commit\", \"repo\", \"filepath\",]).agg({\"token.attention.score\": [\"max\", \"min\", \"std\"]}).reset_index()\n",
    "df_to_plot.columns = [\"datetime\",\"commit\", \"repo\", \"filepath\", \"max\", \"min\", \"sd\"]\n",
    "# df_to_plot.columns = [\"test\", \"filename\", \"token\", \"Range\", \"SD\"]\n",
    "\n",
    "# RQ1-2\n",
    "df_all_copy = df_all.copy()\n",
    "df_all_copy = df_all_copy[(df_all_copy[\"is_comment\"] == \"False\") & (df_all_copy[\"target\"] == \"True\") & (df_all_copy[\"y_pred\"] == \"True\")]\n",
    "\n",
    "clean_lines_df = df_all_copy[df_all_copy[\"is_buggy\"] == \"False\"]\n",
    "buggy_lines_df = df_all_copy[df_all_copy[\"is_buggy\"] == \"True\"]\n",
    "\n",
    "# clean_lines_token_score = clean_lines_df.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": \"min\"}).reset_index()\n",
    "clean_lines_token_score = clean_lines_df.groupby([\"commit\", \"repo\", \"filepath\"]).agg({\"token.attention.score\": \"min\"}).reset_index()\n",
    "clean_lines_token_score[\"class\"] = \"Clean Lines\"\n",
    "\n",
    "# buggy_lines_token_score = buggy_lines_df.groupby([\"test\", \"filename\", \"token\"]).agg({\"token.attention.score\": \"max\"}).reset_index()\n",
    "buggy_lines_token_score = buggy_lines_df.groupby([\"commit\", \"repo\", \"filepath\"]).agg({\"token.attention.score\": \"max\"}).reset_index()\n",
    "buggy_lines_token_score[\"class\"] = \"Defective Lines\"\n",
    "\n",
    "all_lines_token_score = pd.concat([buggy_lines_token_score, clean_lines_token_score])\n",
    "all_lines_token_score[\"class\"] = pd.Categorical(all_lines_token_score[\"class\"], categories=[\"Defective Lines\", \"Clean Lines\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_level_metrics(df_file):\n",
    "    all_gt = df_file[\"target\"]\n",
    "    all_prob = df_file[\"y_pred\"] #prediction.prob\n",
    "    all_pred = df_file[\"target\"] #prediction.label\n",
    "    \n",
    "    confusion_mat = confusion_matrix(all_pred, all_gt)\n",
    "    \n",
    "    bal_acc = confusion_mat[1, 1] / (confusion_mat[1, 1] + confusion_mat[0, 1])\n",
    "    AUC = roc_auc_score(all_gt, all_prob)\n",
    "    \n",
    "    all_pred = np.where(all_pred == \"False\", 0, 1)\n",
    "    all_gt = np.where(all_gt == \"False\", 0, 1)\n",
    "    \n",
    "    MCC = matthews_corrcoef(all_gt, all_pred)\n",
    "    \n",
    "    if np.isnan(MCC):\n",
    "        MCC = 0\n",
    "    \n",
    "    eval_result = [AUC, MCC, bal_acc]\n",
    "    \n",
    "    return eval_result\n",
    "\n",
    "def get_file_level_eval_result(prediction_dir, method_name):\n",
    "    all_files = os.listdir(prediction_dir)\n",
    "\n",
    "    all_auc = []\n",
    "    all_mcc = []\n",
    "    all_bal_acc = []\n",
    "    all_test_rels = []\n",
    "\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(os.path.join(prediction_dir, f))\n",
    "\n",
    "        if method_name == \"DeepLineDP\":\n",
    "            df = df[[\"train\", \"test\", \"filename\", \"target\", \"y_pred\", \"target\"]]\n",
    "            df = df.drop_duplicates()\n",
    "\n",
    "        file_level_result = get_file_level_metrics(df)\n",
    "\n",
    "        AUC = file_level_result[0]\n",
    "        MCC = file_level_result[1]\n",
    "        bal_acc = file_level_result[2]\n",
    "\n",
    "        all_auc.append(AUC)\n",
    "        all_mcc.append(MCC)\n",
    "        all_bal_acc.append(bal_acc)\n",
    "        all_test_rels.append(f.replace(\".csv\", \"\"))\n",
    "\n",
    "    result_df = pd.DataFrame({\"AUC\": all_auc, \"MCC\": all_mcc, \"Balance.Accuracy\": all_bal_acc})\n",
    "\n",
    "    all_test_rels = [rel.replace(\".csv\", \"\") for rel in all_test_rels]\n",
    "\n",
    "    result_df[\"release\"] = all_test_rels\n",
    "    result_df[\"technique\"] = method_name\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# bi_lstm_prediction_dir = \"../output/prediction/Bi-LSTM/\"\n",
    "# cnn_prediction_dir = \"../output/prediction/CNN/\"\n",
    "# dbn_prediction_dir = \"../output/prediction/DBN/\"\n",
    "# lr_prediction_dir = \"../output/prediction/LR/\"\n",
    "\n",
    "# bi_lstm_result = get_file_level_eval_result(bi_lstm_prediction_dir, \"Bi.LSTM\")\n",
    "# cnn_result = get_file_level_eval_result(cnn_prediction_dir, \"CNN\")\n",
    "# dbn_result = get_file_level_eval_result(dbn_prediction_dir, \"DBN\")\n",
    "# lr_result = get_file_level_eval_result(lr_prediction_dir, \"LR\")\n",
    "# deepline_dp_result = get_file_level_eval_result(prediction_dir, \"DeepLineDP\")\n",
    "\n",
    "# all_result = pd.concat([bi_lstm_result, cnn_result, dbn_result, lr_result, deepline_dp_result])\n",
    "\n",
    "# all_result.columns = [\"AUC\", \"MCC\", \"Balance.Accuracy\", \"Release\", \"Technique\"]\n",
    "\n",
    "# auc_result = all_result[[\"Technique\", \"AUC\"]]\n",
    "# auc_result = preprocess(auc_result, False)\n",
    "# auc_result.loc[auc_result[\"variable\"] == \"Bi.LSTM\", \"variable\"] = \"Bi-LSTM\"\n",
    "\n",
    "# mcc_result = all_result[[\"Technique\", \"MCC\"]]\n",
    "# mcc_result = preprocess(mcc_result, False)\n",
    "# mcc_result.loc[mcc_result[\"variable\"] == \"Bi.LSTM\", \"variable\"] = \"Bi-LSTM\"\n",
    "\n",
    "# bal_acc_result = all_result[[\"Technique\", \"Balance.Accuracy\"]]\n",
    "# bal_acc_result = preprocess(bal_acc_result, False)\n",
    "# bal_acc_result.loc[bal_acc_result[\"variable\"] == \"Bi.LSTM\", \"variable\"] = \"Bi-LSTM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ded20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Code for RQ3 -----------------------#\n",
    "\n",
    "def get_line_metrics_result(baseline_df, cur_df_file):\n",
    "    baseline_df_with_ground_truth = pd.merge(baseline_df, cur_df_file, on=[\"filename\", \"line.number\"])\n",
    "\n",
    "    sorted_df = baseline_df_with_ground_truth.groupby(\"filename\").apply(lambda x: x.sort_values(\"line.score\", ascending=False)).reset_index(drop=True)\n",
    "    sorted_df[\"order\"] = sorted_df.groupby(\"filename\").cumcount() + 1\n",
    "\n",
    "    # IFA\n",
    "    IFA = sorted_df[sorted_df[\"line.level.ground.truth\"] == \"True\"].groupby(\"filename\").apply(lambda x: x.nsmallest(1, \"order\")).reset_index(drop=True)\n",
    "\n",
    "    total_true = sorted_df.groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "\n",
    "    # Recall20%LOC\n",
    "    recall20LOC = sorted_df.groupby(\"filename\").apply(lambda x: x[x[\"order\"] <= int(0.2 * len(x))]).groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "    recall20LOC = recall20LOC.merge(total_true, on=\"filename\")\n",
    "    recall20LOC[\"recall20LOC\"] = recall20LOC[\"line.level.ground.truth\"] / recall20LOC[\"line.level.ground.truth_y\"]\n",
    "\n",
    "    # Effort20%Recall\n",
    "    effort20Recall = sorted_df.merge(total_true, on=\"filename\").groupby(\"filename\").apply(lambda x: sum(x[\"line.level.ground.truth\"].cumsum() / x[\"line.level.ground.truth_y\"] <= 0.2) / len(x)).reset_index()\n",
    "\n",
    "    ifa_list = IFA[\"order\"].tolist()\n",
    "    recall_list = recall20LOC[\"recall20LOC\"].tolist()\n",
    "    effort_list = effort20Recall[0].tolist()\n",
    "\n",
    "    result_df = pd.DataFrame({\"ifa_list\": ifa_list, \"recall_list\": recall_list, \"effort_list\": effort_list})\n",
    "\n",
    "    return result_df\n",
    "\n",
    "all_eval_releases = ['activemq-5.2.0', 'activemq-5.3.0', 'activemq-5.8.0', 'camel-2.10.0', 'camel-2.11.0', 'derby-10.5.1.1', 'groovy-1_6_BETA_2', 'hbase-0.95.2', 'hive-0.12.0', 'jruby-1.5.0', 'jruby-1.7.0.preview1', 'lucene-3.0.0', 'lucene-3.1', 'wicket-1.5.3']\n",
    "\n",
    "error_prone_result_dir = '../output/ErrorProne_result/'\n",
    "ngram_result_dir = '../output/n_gram_result/'\n",
    "rf_result_dir = '../output/RF-line-level-result/'\n",
    "\n",
    "n_gram_result_df = pd.DataFrame()\n",
    "error_prone_result_df = pd.DataFrame()\n",
    "rf_result_df = pd.DataFrame()\n",
    "\n",
    "for rel in all_eval_releases:\n",
    "    error_prone_result = pd.read_csv(os.path.join(error_prone_result_dir, rel + '-line-lvl-result.txt'), quotechar=\"\")\n",
    "    error_prone_result[\"EP_prediction_result\"] = error_prone_result[\"EP_prediction_result\"].replace({\"False\": 0, \"True\": 1})\n",
    "\n",
    "    n_gram_result = pd.read_csv(os.path.join(ngram_result_dir, rel + '-line-lvl-result.txt'), quotechar=\"\")\n",
    "    rf_result = pd.read_csv(os.path.join(rf_result_dir, rel + '-line-lvl-result.csv'))\n",
    "\n",
    "    n_gram_result = n_gram_result[[\"filename\", \"line.number\", \"line.score\"]]\n",
    "    rf_result = rf_result[[\"filename\", \"line_number\", \"line.score.pred\"]]\n",
    "\n",
    "    cur_df_file = df_all[df_all[\"test\"] == rel]\n",
    "    cur_df_file = cur_df_file[[\"filename\", \"line.number\", \"line.level.ground.truth\"]]\n",
    "\n",
    "    n_gram_eval_result = get_line_metrics_result(n_gram_result, cur_df_file)\n",
    "    error_prone_eval_result = get_line_metrics_result(error_prone_result, cur_df_file)\n",
    "    rf_eval_result = get_line_metrics_result(rf_result, cur_df_file)\n",
    "\n",
    "    n_gram_result_df = pd.concat([n_gram_result_df, n_gram_eval_result])\n",
    "    error_prone_result_df = pd.concat([error_prone_result_df, error_prone_eval_result])\n",
    "    rf_result_df = pd.concat([rf_result_df, rf_eval_result])\n",
    "\n",
    "# Force attention score of comment line is 0\n",
    "df_all.loc[df_all[\"is.comment.line\"] == \"True\", \"token.attention.score\"] = 0\n",
    "\n",
    "tmp_top_k = get_top_k_tokens(df_all, 1500)\n",
    "\n",
    "merged_df_all = pd.merge(df_all, tmp_top_k, on=[\"project\", \"train\", \"test\", \"filename\", \"token\"], how=\"left\")\n",
    "merged_df_all.loc[merged_df_all[\"flag\"].isna(), \"token.attention.score\"] = 0\n",
    "\n",
    "sum_line_attn = merged_df_all[(merged_df_all[\"file.level.ground.truth\"] == \"True\") & (merged_df_all[\"prediction.label\"] == \"True\")]\n",
    "sum_line_attn = sum_line_attn.groupby([\"test\", \"filename\", \"is.comment.line\", \"file.level.ground.truth\", \"prediction.label\", \"line.number\", \"line.level.ground.truth\"]).agg({\"token.attention.score\": \"sum\", \"num_tokens\": \"count\"}).reset_index()\n",
    "\n",
    "sorted_df = sum_line_attn.groupby([\"test\", \"filename\"]).apply(lambda x: x.sort_values(\"token.attention.score\", ascending=False)).reset_index(drop=True)\n",
    "sorted_df[\"order\"] = sorted_df.groupby([\"test\", \"filename\"]).cumcount() + 1\n",
    "\n",
    "# get result from DeepLineDP\n",
    "# calculate IFA\n",
    "IFA = sorted_df[sorted_df[\"line.level.ground.truth\"] == \"True\"].groupby([\"test\", \"filename\"]).apply(lambda x: x.nsmallest(1, \"order\")).reset_index(drop=True)\n",
    "\n",
    "total_true = sorted_df.groupby([\"test\", \"filename\"]).agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "\n",
    "# calculate Recall20%LOC\n",
    "recall20LOC = sorted_df.groupby([\"test\", \"filename\"]).apply(lambda x: x[x[\"order\"] <= int(0.2 * len(x))]).groupby([\"test\", \"filename\"]).agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "recall20LOC = recall20LOC.merge(total_true, on=[\"test\", \"filename\"])\n",
    "recall20LOC[\"recall20LOC\"] = recall20LOC[\"line.level.ground.truth\"] / recall20LOC[\"line.level.ground.truth_y\"]\n",
    "\n",
    "# calculate Effort20%Recall\n",
    "effort20Recall = sorted_df.merge(total_true, on=[\"test\", \"filename\"]).groupby([\"test\", \"filename\"]).apply(lambda x: sum(x[\"line.level.ground.truth\"].cumsum() / x[\"line.level.ground.truth_y\"] <= 0.2) / len(x)).reset_index()\n",
    "\n",
    "# prepare data for plotting\n",
    "deeplinedp_ifa = IFA[\"order\"].tolist()\n",
    "deeplinedp_recall = recall20LOC[\"recall20LOC\"].tolist()\n",
    "deeplinedp_effort = effort20Recall[0].tolist()\n",
    "\n",
    "deepline_dp_line_result = pd.DataFrame({\"IFA\": deeplinedp_ifa, \"Recall20%LOC\": deeplinedp_recall, \"Effort@20%Recall\": deeplinedp_effort})\n",
    "\n",
    "rf_result_df.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "n_gram_result_df.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "error_prone_result_df.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "deepline_dp_line_result.columns = [\"IFA\", \"Recall20%LOC\", \"Effort@20%Recall\"]\n",
    "\n",
    "rf_result_df[\"technique\"] = \"RF\"\n",
    "n_gram_result_df[\"technique\"] = \"N.gram\"\n",
    "error_prone_result_df[\"technique\"] = \"ErrorProne\"\n",
    "deepline_dp_line_result[\"technique\"] = \"DeepLineDP\"\n",
    "\n",
    "all_line_result = pd.concat([rf_result_df, n_gram_result_df, error_prone_result_df, deepline_dp_line_result])\n",
    "\n",
    "recall_result_df = all_line_result[[\"technique\", \"Recall20%LOC\"]]\n",
    "ifa_result_df = all_line_result[[\"technique\", \"IFA\"]]\n",
    "effort_result_df = all_line_result[[\"technique\", \"Effort@20%Recall\"]]\n",
    "\n",
    "recall_result_df = preprocess(recall_result_df, False)\n",
    "ifa_result_df = preprocess(ifa_result_df, True)\n",
    "effort_result_df = preprocess(effort_result_df, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Code for RQ4 -----------------------#\n",
    "\n",
    "# get within-project result\n",
    "deepline_dp_result[\"project\"] = ['activemq', 'activemq', 'activemq', 'camel', 'camel', 'derby', 'groovy', 'hbase', 'hive', 'jruby', 'jruby', 'lucene', 'lucene', 'wicket']\n",
    "\n",
    "file_level_by_project = deepline_dp_result.groupby(\"project\").agg({\"all.auc\": \"mean\", \"all.mcc\": \"mean\", \"all.bal.acc\": \"mean\"}).reset_index()\n",
    "file_level_by_project.columns = [\"project\", \"AUC\", \"MCC\", \"Balance Accurracy\"]\n",
    "\n",
    "# get cross-project result\n",
    "prediction_dir = '../output/prediction/DeepLineDP/cross-release/'\n",
    "\n",
    "projs = ['activemq', 'camel', 'derby', 'groovy', 'hbase', 'hive', 'jruby', 'lucene', 'wicket']\n",
    "\n",
    "def get_line_level_metrics(df_all):\n",
    "    sum_line_attn = df_all[(df_all[\"file.level.ground.truth\"] == \"True\") & (df_all[\"prediction.label\"] == \"True\")]\n",
    "    sum_line_attn = sum_line_attn.groupby(\"filename\").agg({\"token.attention.score\": \"sum\", \"num_tokens\": \"count\"}).reset_index()\n",
    "\n",
    "    sorted_df = sum_line_attn.groupby(\"filename\").apply(lambda x: x.sort_values(\"token.attention.score\", ascending=False)).reset_index(drop=True)\n",
    "    sorted_df[\"order\"] = sorted_df.groupby(\"filename\").cumcount() + 1\n",
    "\n",
    "    # calculate IFA\n",
    "    IFA = sorted_df[sorted_df[\"line.level.ground.truth\"] == \"True\"].groupby(\"filename\").apply(lambda x: x.nsmallest(1, \"order\")).reset_index(drop=True)\n",
    "\n",
    "    total_true = sorted_df.groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "\n",
    "    # calculate Recall20%LOC\n",
    "    recall20LOC = sorted_df.groupby(\"filename\").apply(lambda x: x[x[\"order\"] <= int(0.2 * len(x))]).groupby(\"filename\").agg({\"line.level.ground.truth\": lambda x: sum(x == \"True\")}).reset_index()\n",
    "    recall20LOC = recall20LOC.merge(total_true, on=\"filename\")\n",
    "    recall20LOC[\"recall20LOC\"] = recall20LOC[\"line.level.ground.truth\"] / recall20LOC[\"line.level.ground.truth_y\"]\n",
    "\n",
    "    # calculate Effort20%Recall\n",
    "    effort20Recall = sorted_df.merge(total_true, on=\"filename\").groupby(\"filename\").apply(lambda x: sum(x[\"line.level.ground.truth\"].cumsum() / x[\"line.level.ground.truth_y\"] <= 0.2) / len(x)).reset_index()\n",
    "\n",
    "    all_ifa = IFA[\"order\"].tolist()\n",
    "    all_recall = recall20LOC[\"recall20LOC\"].tolist()\n",
    "    all_effort = effort20Recall[0].tolist()\n",
    "\n",
    "    result_df = pd.DataFrame({\"all.ifa\": all_ifa, \"all.recall\": all_recall, \"all.effort\": all_effort})\n",
    "\n",
    "    return result_df\n",
    "\n",
    "all_line_result = pd.DataFrame()\n",
    "all_file_result = pd.DataFrame()\n",
    "\n",
    "for p in projs:\n",
    "    actual_pred_dir = os.path.join(prediction_dir, p)\n",
    "\n",
    "    all_files = os.listdir(actual_pred_dir)\n",
    "\n",
    "    all_auc = []\n",
    "    all_mcc = []\n",
    "    all_bal_acc = []\n",
    "    all_src_projs = []\n",
    "    all_tar_projs = []\n",
    "\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(os.path.join(actual_pred_dir, f))\n",
    "\n",
    "        f = f.replace(\".csv\", \"\")\n",
    "        f_split = f.split(\"-\")\n",
    "        target = f_split[-2]\n",
    "\n",
    "        df_file = df[[\"train\", \"test\", \"filename\", \"file.level.ground.truth\", \"prediction.prob\", \"prediction.label\"]]\n",
    "        df_file = df_file.drop_duplicates()\n",
    "\n",
    "        file_level_result = get_file_level_metrics(df_file)\n",
    "\n",
    "        AUC = file_level_result[0]\n",
    "        MCC = file_level_result[1]\n",
    "        bal_acc = file_level_result[2]\n",
    "\n",
    "        all_auc.append(AUC)\n",
    "        all_mcc.append(MCC)\n",
    "        all_bal_acc.append(bal_acc)\n",
    "\n",
    "        all_src_projs.append(p)\n",
    "        all_tar_projs.append(target)\n",
    "\n",
    "        tmp_top_k = get_top_k_tokens(df, 1500)\n",
    "\n",
    "        merged_df_all = pd.merge(df, tmp_top_k, on=[\"project\", \"train\", \"test\", \"filename\", \"token\"], how=\"left\")\n",
    "        merged_df_all.loc[merged_df_all[\"flag\"].isna(), \"token.attention.score\"] = 0\n",
    "\n",
    "        line_level_result = get_line_level_metrics(merged_df_all)\n",
    "        line_level_result[\"src\"] = p\n",
    "        line_level_result[\"target\"] = target\n",
    "\n",
    "        all_line_result = pd.concat([all_line_result, line_level_result])\n",
    "\n",
    "    file_level_result = pd.DataFrame({\"all.auc\": all_auc, \"all.mcc\": all_mcc, \"all.bal.acc\": all_bal_acc})\n",
    "    file_level_result[\"src\"] = p\n",
    "    file_level_result[\"target\"] = all_tar_projs\n",
    "\n",
    "    all_file_result = pd.concat([all_file_result, file_level_result])\n",
    "\n",
    "final_file_level_result = all_file_result.groupby(\"target\").agg({\"all.auc\": \"mean\", \"all.bal.acc\": \"mean\", \"all.mcc\": \"mean\"}).reset_index()\n",
    "final_line_level_result = all_line_result.groupby(\"target\").agg({\"all.recall\": \"mean\", \"all.effort\": \"mean\", \"all.ifa\": \"mean\"}).reset_index()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
